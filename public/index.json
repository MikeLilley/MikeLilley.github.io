[{"content":" Introduction If you're familiar at all with physics, you might have heard the term tensor get thrown around from time to time, perhaps in the context of the stress-energy tensor. In the Fall of 2019, I registered for my second semester of a two-part course my university offered on data structures and algorithms. My now late professor was very exhuberent about the development of quantum computing, and spent some time at the beginning of the term talking about the fundamentals. If you're unfamilar, the basic idea of quantum computing is that if we utilize the properties that systems of particles have to store and act as processing pipelines of information, we can exhibit massive computational speedups for a select few imporant problems. This is a paradigm at odds with classical computation, whose operation primarily relies on digital signal processing and transistor circuits. Quantum computing and quantum circuits are typically described in the language of linear algebra. The analogous notion of information to the bit, the qubit, is represented as a unit vector in \\(\\mathbb{C}^2,\\) and logic gates are typically constructed as square matrices, since these induce an endomorphism (read: transformation which maps elements of a vector space back onto that same vector space). As mentioned before, much of the power which comes from quantum models is a result of considering systems of particles in an ensemble; typically in a way such that they are coupled to one another. This coupling comes in the form of quantum entanglement, a situation in which the quantum state of an entangled particle cannot be described independently of the other particles it is entangled with. We describe this coupling (but not yet entanglement) mathematically utilizing an operation \\(\\otimes\\) called the Kronecker product, which is a generalization of the outer product between vectors. This operation takes two matrices of arbitrary size and produces a block matrix.\nDefinition 1.1: Let A be a \\(m \\times n\\) matrix and B be a \\(p \\times q\\) matrix; then, the Kronecker Product \\(\\otimes\\) applied as \\(A \\otimes B\\) produces a \\(pm \\times qn\\) matrix as follows: $$A \\otimes B = \\begin{bmatrix} a_{11}B \u0026amp; \\dots \u0026amp; a_{1n}B \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ a_{m1}B \u0026amp; \\dots \u0026amp; a_{mn}B \\end{bmatrix}$$\nUsing two qubits as an example, if \\(A = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} \\) and \\(B = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} ,\\) then \\(A \\otimes B \\) = \\(\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix} .\\)\nThe Kronecker product is a matrix representation of a much more generalizable operation called the tensor product, which is the more common name seen in literature. Tensors are particularly challenging to understand at first, mostly as a result of their generality. I found that different sources provided definitions at different levels of abstraction, and finding a general definition which encapsulated all of these wasn't trivial.\nIn this post, we'll be restricting ourselves to finite-dimensional vector spaces. Despite the already immense generality that exists here, we can go even further and start talking about vector spaces of infinite dimensions or tensors with respect to modules, the construction of which relies on quite a bit of category theory. In addition, we will also be assuming that we can construct the bases of any vector spaces we describe in the section on the tensor product. Making these arguments without explicitly referring to bases is possible, but requires some mental gymnastics. A First Look at Tensors Definition 2.1: Let \\(V = \\lbrace V_1, V_2, \\cdots, V_n, W \\rbrace \\) be a set of vector spaces over a common field \\(\\mathbb{F}.\\) A tensor is a multilinear map \\(f: V_1 \\times \\cdots \\times V_n \\rightarrow W .\\)\nRecall that multilinear means the mapping is linear in each variable. That is to say, if we fix all other variables but one, the mapping is linear (also known as a one-form). A canonical example of a tensor is a linear functional over a vector space. A linear functional maps from a vector space back to its ground field, and since a field is a vector space over itself we can say that a linear functional constitutes a tensor.\nFor instance, consider the vector space \\(\\mathbb{R}^3 .\\) Denoting elements of \\(\\mathbb{R}^3 \\) by column vectors, we may say that the linear functionals consist of all real-valued \\(1 \\times 3 \\) row vectors. The set of linear functionals of a vector space is also known as its dual space. As a result, all elements of the dual space of a vector space are tensors. Conversely, all elements of a vector space are tensors, as they map elements of their dual space back to the ground field. For another example, consider the vector space \\(M_n (\\mathbb{F}) .\\) All elements of this vector space are tensors, as they are linear operators which map from \\( \\mathbb{F}^n \\) to \\( \\mathbb{F}^n .\\) As was mentioned before, tensors do not need to be represented by a matrix; they simply need to be a multilinear map. For a final example, consider the dot product as a tensor; if we fix one of the operands, the dot product becomes a linear transformation. We will now define the tensor product, which as described earlier, is a generalization of the Kronecker product, as it is defined in terms of vector spaces and mappings, rather than just matrices. Note that the tensor product and Kronecker product share the same symbol.\nDefinition 2.2: Let \\(V_1 \\) and \\(V_2 \\) be vector spaces over a common field \\(\\mathbb{F}.\\) The result of the tensor product \\(V_1 \\otimes V_2 \\) is the vector space formed by quotienting the free module \\( F(V_1 \\times V_2) \\) by a relation \\(\\sim ,\\) defined by: Identity:\\(\\ (v,w) \\sim (v,w)\\) Symmetry: \\( (v,w) \\sim (v',w') \\implies (v',w') \\sim (v,w)\\) Transitivity: \\( \\ (v,w) \\sim (v',w') \\mbox{ and } (v',w') \\sim (v'',w'') \\implies (v,w) \\sim (v'',w'')\\) Distribution: \\( (v,w) + (v',w) \\sim (v + v',w') \\mbox{ and } (v,w) + (v,w') \\sim (v,w + w')\\) Scalar Multiples: \\( c(v,w) \\sim (cv,w) \\mbox{ and } c(v,w) \\sim (v,cw)\\) where \\( (v,w) ,\\)\\( (v',w') \\in F(V_1 \\times V_2) .\\) So, \\(V_1 \\otimes V_2 := F(V_1 \\times V_2) \\backslash \\sim .\\) Using Definition 2.2, we may redefine the tensor using a different, but equivalent definition:\nDefinition 2.3: Let \\(S = \\lbrace V_1, V_2, \\cdots, V_n \\rbrace \\) be a set of vector spaces over a common field \\(\\mathbb{F}.\\) We define a tensor as an element of the tensor product \\(V_1 \\otimes V_2 \\otimes \\cdots \\otimes V_n .\\) A tensor on the vector space \\(V \\) is defined to be an element of the tensor product \\(V \\otimes \\cdots \\otimes V \\otimes V^* \\otimes \\cdots \\otimes V^* ,\\) where \\(V^* \\) is dual space of \\(V .\\)\nRecall that the dual space \\(V^* \\) of a vector space \\(V \\) is the vector space over the same field \\( \\mathbb{F} \\) whose elements is the set of one-forms on \\(V ,\\) where a one-form is a linear mapping from a vector space to its ground field. We often say that tensors corresponding to the second definition are of type \\( (m,n) ,\\) where \\(m \\) is the number of copies of \\(V \\) and \\(n \\) is the number of copies of \\(V^* .\\) A tensor of type \\( (1,0) \\) is just a vector.\nWe may also define the tensor product between linear maps.\nDefinition 2.4: Let \\(F: W \\rightarrow Y \\) and \\(G: X \\rightarrow Z \\) be linear maps. The result of the tensor product \\(F \\otimes G \\) is the linear map \\(F \\otimes G: W \\otimes X \\rightarrow Y \\otimes Z .\\) The tensor product is denoted so because it provides a natural correspondence between mutltilinear maps and linear maps, allowing us to reduce problems of multilinear algebra to linear algebra. This relationship is actually injective; given by something called the universal property, which will we will state without proof: Theorem 2.5 (universal property): Let \\( L(V_1, V_2, \\cdots, V_p; V) \\) be the set of multilinear functions which map between \\(V_1 \\times \\cdots \\times V_n \\rightarrow V .\\) For any multilinear functional \\(F \\in L(V_1, V_2, \\cdots, V_p; V) ,\\) there exists a unique linear transformation T such that \\( F(v_1, v_2, \\cdots, v_p) = T(v_1 \\otimes v_2 \\otimes \\cdots \\otimes v_p) .\\)\nIn matrices, this transformation is given immediately by a property of the Kronecker product. If we consider A and B as matrices of linear maps and if \\(A \\) and \\(B \\) are matrices, and \\(v \\) and \\(w \\) are vectors, then \\(Av \\otimes Bw \\) = \\((A \\otimes B)(v \\otimes w) .\\)\nInterpreting Tensors in a Physical Sense Tensors are often characterized by the fact that their components exhibit certain transformations under a change of coordinates. These transformations are a particularly important property when applying tensors to various physical problems. In this section, we'll be examining these transformations and why they are important.\nA common procedure to perform when working in a vector space is a change of basis. As the name suggests, this is a procedure in which we change the representation of a given vector space \\(V \\) by means of constructing a different set of basis vectors. Understanding how multilinear transformations (and their notational representations) transform under a change of basis is key to understanding how to apply them in other coordinate systems. To motivate these transforms, consider the following example:\nExample 4.1: Consider the vector space \\(\\mathbb{R}^2 ,\\) represented by the standard basis vectors. If we expand the vector \\(A = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \\) into its coefficients and basis vectors, we have:\n$$A = a_1 \\hat{i} + a_2\\hat{j} = 2 \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} + 4 \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} $$\nIf we perform a change of basis so that our new basis is \\(\\mathcal{B} = \\bigg\\lbrace \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} \\bigg\\rbrace ,\\) we would instead expand \\(A \\) as: $$A = (a_1/2) \\mathcal{B}_1 + (a_2/2) \\mathcal{B}_2 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} + 2 \\begin{bmatrix} 0 \\\\ 2 \\end{bmatrix} $$\nBy doubling our basis vectors, we had to halve our coefficients in order to represent the same vector in \\(\\mathbb{R}^2 .\\) Generally, coefficients always change contrary to the basis vectors under a change of basis.\nNow, consider the function \\(f = \\sin(x) + y^2\\) and its gradient \\(\\nabla f = \\cos(x) + 2y.\\) Under that same change of basis, we can represent the old coordinates as a function of the new coordinates, and therefore describe the gradient in the old coordinates as a function of the gradient in the new coordinates. Let \\(x' = 2x \\) and \\(y' = 2y ,\\) (where \\(x'\\) and \\(y'\\) are the terms representative of the new bases) so that the function expressed in the new basis as \\(f = \\sin(x') + (y')^2 .\\)\nSubstituting yields \\(f = \\sin(2x) + (2y)^2.\\) Now if we take the gradient, we get:\n$$\\nabla f = 2\\cos(2x)+ 2(2y) = 2\\cos(x\u0026rsquo;) + 2(y\u0026rsquo;) $$\nFollowing a scaling of the basis, we find that the gradient of \\(f \\) also scales in the same manner, which is the opposite behavior we observe compared to the coordinates. This example illustrates the two primary types of transformations which occur to components of a tensor following a change of basis: contravariant and covariant. Components which transform in a contravariant manner do so contrary to the transformation which the basis vectors undergo, whereas those which do so in a covariant manner do so in the same manner as the basis vectors.\nTensors can have components which are entirely contravariant, covariant, or a mix of the two. To indicate which components transform in which manner, we use a mix of subscript and superscript notation, with the former corresponding to contravariant components and the latter to covariant components. For instance, we would write the vector \\(A \\) as follows:\n$$A = \\sum_{i}a^iB_i = a^1 \\hat{i} + a^2\\hat{j} $$\nThis style is known as Einstein notation.\nThe Cauchy Stress Tensor There are many important tensors that are used in the mathematical sciences; one such example is the Cauchy Stress Tensor, found in continuum mechanics. This tensor describes the stress at any point in a solid object, and has components that transform contravariantly. While representative of an object in three dimensions, the tensor has nine values. One might reason that we only need three values corresponding to the stresses in the \\(x ,\\) \\(y ,\\) and \\(z \\) directions, but physical stress does not adhere to standard vector addition rules; if I push on a box using the same force from two opposite sides, the total stress on the box is not zero.\nFigure 3.1: The Cauchy Stress Tensor; Adapted From Sanpaz\nTo model stress at a point, we aim to describe the three-dimensional stress an object could undergo at each of its faces. To treat this in a semi-formal manner, consider splitting our object into small cubes, with one such cube denoted by the tuple \\( C = (s_{x,1}, s_{x,2}, s_{y,1}, s_{y,2}, s_{z,1}, s_{z,2}) \\) where \\(s_{x,1} \\) and \\(s_{x,2}\\) are stresses on directly opposite faces perpendicular to the x-axis, and the face area denoted by \\(\\Delta A .\\) By physical experimentation, it has been found that: $$ \\lim_{\\Delta A \\rightarrow 0} s_{i,1} - s_{i,2} = 0 , \\ i = x,y,z$$\nThis observation is called Cauchy's Fundamental Lemma of Continuum Mechanics. As a result, we only need 9 numbers rather than 18 to hold the information of stress on all faces at a particular point in an object, as \\(s_{i,1} = - s_{i,2} .\\) Conventionally, the stress tensor is denoted as:\n$$\\sigma = \\begin{bmatrix} \\sigma_{x,x} \u0026amp; \\sigma_{x,y} \u0026amp; \\sigma_{x,z} \\\\ \\sigma_{y,x} \u0026amp; \\sigma_{y,y} \u0026amp; \\sigma_{y,z} \\\\ \\sigma_{z,x} \u0026amp; \\sigma_{z,y} \u0026amp; \\sigma_{z,z} \\end{bmatrix} $$\nTo retrieve the stress at a particular face, we simply need to take the dot product of the stress tensor with a unit row vector. For instance: $$ \\begin{bmatrix} 1 \u0026 0 \u0026 0 \\\\ \\end{bmatrix} \\ \\cdot \\begin{bmatrix} \\sigma_{x,x} \u0026 \\sigma_{x,y} \u0026 \\sigma_{x,z} \\\\ \\sigma_{y,x} \u0026 \\sigma_{y,y} \u0026 \\sigma_{y,z} \\\\ \\sigma_{z,x} \u0026 \\sigma_{z,y} \u0026 \\sigma_{z,z} \\end{bmatrix} = \\begin{bmatrix} \\sigma_{x,x} \u0026 \\sigma_{x,y} \u0026 \\sigma_{x,z} \\end{bmatrix} $$\nSince the exact coefficients of the stress tensor are dependent upon the material of the object as well as the exact types of forces that are being applied, we will not go through a computation for this, and will end the post here. Despite this, it is my hope that you have found this to be a useful introduction to tensors.\nReferences [1] Irgens, Fridtjov. Continuum mechanics. Springer Science \u0026 Business Media, 2008.\n[2] Treil, Sergi. \"Linear algebra done wrong.\" (2016).\n[3] Treves, François. Topological Vector Spaces, Distributions and Kernels: Pure and Applied Mathematics, Vol. 25. Vol. 25. Elsevier, 2016.\n[4] Stover, Christopher and Weisstein, Eric W. \"Einstein Summation.\" From MathWorld--A Wolfram Web Resource.\n","permalink":"https://MikeLilley.github.io/posts/structure-tensor/","summary":"In this post, we\u0026rsquo;ll be taking a brief look at tensors. These can be confusing objects to understand, but are central to many areas in mathematics, physics, and computer science.","title":"A Primer on Tensors"},{"content":" Introduction In topology, we study objects contained in topological spaces with respect to something called a homeomorphism. Definition 1.1: Let \\(X\\) and \\(Y\\) be topological spaces. \\(X\\) and \\(Y\\) are said to be homeomorphic (or topologically equivalent) if there exists a continuous bijective function \\(f: X \\rightarrow Y\\) that maps all points in \\(X\\) to points in \\(Y,\\) such that the inverse of the function is also continuous. The map \\(f\\) is called the homeomorphism. Visually speaking, a homeomorphism can be thought of as a continuous mapping that stretches and bends a topological space into another, without any sort of tearing or gluing. The existence of homeomorphisms are an important topic of study because they preserve all properties of a topology, and can show that two seemingly-different spaces are actually equivalent. Homeomorphisms between topological spaces are akin to isomorphisms between algebraic structures.\nHoles are one example of an invariant topological property under homeomorphism. This means that we have to ensure that both of our topological spaces have the same amount of holes before attempting to construct a homeomorphism from one to the other. For this reason, we often group topological spaces together by their hole count, or genus.\nIt is not easy to classify the genus of most topological objects, since what constitutes a hole is subject to discretion at first glance. This is especially pronounced in high-dimensional spaces. It wasn\u0026rsquo;t until the late 1800s that a convenient theory for hole-counting was developed, when Poincaré published his seminal essay, Analysis Situs. In this, he laid out the foundations for homotopy and homology, two distinct but closely-related theories for recognizing and counting holes within topological objects. We will focus on the latter, but we will introduce the definition of a homotopy in the next section.\nPreliminary Definitions Before we go any further, we must state some definitions.\nDefinition 2.1: A connected space is a topological space that cannot be represented as the union of two or more disjoint non-empty open subsets.\nDefinition 2.2: A path in a topological space \\(X\\) is a continuous function \\(f\\) from the unit interval \\(I = [0,1]\\) to \\(X\\); \\(f: I \\rightarrow X.\\) \\(f(0)\\) and \\(f(1)\\) are the start and terminal points of the path, respectively.\nDefinition 2.3: A topological space \\(X\\) is said to be a path-connected space if there exists a path \\(f\\) joining any two arbitrary points in \\(X.\\)\nDefinition 2.4: A homotopy between two paths \\(f\\) and \\(g\\) is a continuous function \\(H: X \\times [0, 1] \\rightarrow Y\\) such that \\(H(x, 0) = f(x)\\) and \\(H(x, 1) = g(x).\\) Definition 2.5: A simply connected space is a topological space \\(X\\) where a homotopy exists for any arbitrary two paths with \\(a, b \\in X\\) as endpoints.\nFigure 2.6: Examples of Paths and Homotopies; Adapted From Archibald\nIf \\(X\\) is not a simply-connected space, then we say that holes exist in \\(X.\\) These are the spaces we are interested in studying. This idea of stretching paths over a topological space gives us a means to know if holes exist within a topological space, but as stated in the prior section, our goal is to count how many holes there are. As it turns out, this is a much harder problem. Individual holes are hard to identify directly since they exist outside of our topological space. However, one thing we can do is attempt to quantify and capture the topological structure that exists around our holes, and count those structures instead.\nDefinition 2.7: A loop in a topological space \\(X\\) is a path \\(f\\) such that \\(f(0) = f(1).\\) In other words, a loop is a path whose starting point is equal to its ending point.\nConsider a sheet of paper. We typically characterize a hole in the paper as being distinct from a tear if the hole is surrounded by a closed loop of the object's material. The essence of this physical analogy is what homology exploits in order to identify holes. To count holes, we should count the amount of loops in a topological space.\nMotivating Homology with Graphs Now that we have the fundamental definitions out of the way, it proves useful to work in a specific topological space in order to develop the rest of our intuition as well as some mathematical tools we will be working with. We will work in the graph topology, since it is easy to understand and illustrate.\nDefinition 3.1: The graph topology \\(T\\) is the topological space which arises from a normal graph \\(G = (V,E)\\) by replacing each vertex \\(v \\in V\\) by a point in \\(\\mathbb{R}\\) and each edge \\(e \\in E\\) by a copy of the unit interval \\(I = [0,1].\\) Figure 3.2: A Three-Node Graph Let \\(G\\) be the directed graph illustrated in Figure 3.2 with the graph topology. We will denote the set \\(V^* \\subseteq T\\) as the labeled copies of the unit intervals which correspond to each edge in \\(G,\\) as well as the set \\(E^* \\subseteq T\\) as the points in \\(T\\) which correspond to the vertices of \\(G.\\)\nNaturally, a path in \\(T\\) can be thought of as a walk on the graph from one vertex to another via their adjacent edge. Since we can walk all around \\(G,\\) it is meaningful to define path composition in \\(T.\\)\nDefintion 3.3: Let \\(P_1, P_2 \\in E^*,\\) where \\(P_1(1) = P_2(0)\\) or \\(P_1(0) = P_2(1).\\) Path composition is a binary operation \\(\\circ\\) on \\(P_1\\) and \\(P_2\\) which results in a mapping of the unit interval \\(I\\) onto \\(P_1 \\cup P_2.\\)\nIn this manner, we treat \\(P_1\\) and \\(P_2\\) as one conjoined path within our graph topology. If \\(P_1(x) = P_2(y),\\) the edge will have endpoint vertices \\(P_1(y)\\) and \\(P_2(x),\\) where \\(x, y \\in \\lbrace 0, 1 \\rbrace.\\) An exception to the above condition is path composition with the identity path \\(0 \\in E^*,\\) where \\(P_1 \\circ 0 = P_1.\\) It is useful to carry the structure of directedness on edges over to our topological space when speaking of path composition, as we can then generate paths in \\(E^*\\) that are the opposite direction of an edge in \\(E.\\) As an example with additive notation, consider \\(C \\in T.\\) \\(C(0) = z,\\) while \\(C(1) = x.\\) If we invert the path, \\(-C(0) = x,\\) whereas \\(-C(1) = z.\\)\nAs an example, \\(A \\circ B\\) and \\(-A \\circ C \\circ -B \\) would be examples of valid paths; \\(A \\circ -B \\) and \\(B \\circ D \\circ C \\) would be undefined paths using path composition per our constraint that \\(P_1(1) = P_2(0)\\) or \\(P_1(0) = P_2(1).\\) Notice that \\(\\circ\\) is a commutative operation. \\(A \\circ B\\) and \\(B \\circ A\\) both yield a path that connects \\(A\\) and \\(B\\) with endpoints \\(A(0)\\) and \\(B(1).\\) In fact, \\(F_1 = (E^*, \\circ)\\) actually forms a free abelian group, a fact that we will end up exploiting later on. We will assume this without proof.\nDefintion 3.4: A free abelian group is defined as an abelian group that contains a set of basis elements.\nIn addition to \\((E^*, \\circ),\\) let us construct the free abelian group \\(F_0 = (V^*, +),\\) which we will define as the set of linear combinations of elements \\(v \\in V^*.\\)\nNow we are ready to introduce the fundamentals of loops in a homological sense. Recall that a boundary \\(\\partial (S)\\) of a subset \\(S\\) in a topological space \\(X\\) is the set of all points \\(x \\in Cl(S) - Int(S).\\)\nOn our graph topology, we will define the boundary operator \\(\\partial : F_1 \\rightarrow F_0\\) as a homomorphism which maps a path \\(P \\in E^*\\) to a linear combination of its endpoints of the form \\(P(1) - P(0),\\) its terminal and starting points.\nDefinition 3.5: On our graph topology, we will define the boundary operator \\(\\partial : F_1 \\rightarrow F_0\\) as a homomorphism which maps a path \\(P \\in E^*\\) to a linear combination of its endpoints of the form \\(P(1) - P(0),\\) its terminal and starting points.\nFor example, consider the labeled edge \\(A \\in E^*:\\) \\(\\partial (A) = y - x.\\)\nDefining \\(F_0\\) as a group of linear combinations of vertices was not an unbiased decision, as it provides a means to show that our boundary operator is distributive over path composition. Most importantly, it allows us to do arithmetic with our vertices.\nConsider \\(\\partial (A \\circ B) = (y - x) + (z - y) = (z - x).\\) Using our boundary operator, we may define a loop as any path \\(P \\in E^*\\) such that \\(\\partial (P) = 0.\\) Loops in the graph topology are also known as cycles, and we may define a subgroup \\(C \\leq E^*\\) of all cycles in \\(E^*\\) known as the cycle space.\nConsider the following three cycles in \\(C,\\) which we can confirm are indeed cycles by applying our boundary operator:\n\\[ \\begin{array}{|c|c|} \\hline \\textbf{Composition} \u0026 \\textbf{Application of Boundary Operator} \\\\ \\hline (C \\circ -D) \u0026 \\partial (C \\circ -D) = (x - z) - (x - z) = 0 \\\\ \\hline (A \\circ B \\circ C) \u0026 \\partial (A \\circ B \\circ C) = (y - x) + (z - y) + (x - z) = 0 \\\\ \\hline (A \\circ B \\circ D) \u0026 \\partial (A \\circ B \\circ D) = (y - x) + (z - y) + (x - z) = 0 \\\\ \\hline \\end{array} \\] So, by simply saying that the total amount of holes in \\(T\\) corresponds to the cardinality of \\(C,\\) we end up with an infinite amount of holes. Obviously, this proves to be not very useful. However, one thing to do is to apply the simple restriction of linear independence to our cycle space to give us a finite hole count. Let \\(H: G \\rightarrow \\mathbb{Z}\\) be the hole-counting function for some arbitrary graph \\(G.\\) We define \\(H(G)\\) to be equivalent to the amount of linearly independent cyclic paths in \\(C.\\) Before we move forward, note that linear independence is not presently well-defined on our path space \\(C.\\)\nSince \\(C\\) is a subgroup of \\(E^*,\\) \\(C\\) is also a free abelian group. Since we are looking for a count of linearly independent elements, this corresponds to the total amount of basis elements in \\(C.\\) To find the corresponding basis elements in \\(C,\\) consider an arbitrary path \\((\\alpha A \\circ \\beta B \\circ \\gamma C \\circ \\delta D).\\) Note that the coefficients associated with our edges simply serve as shorthand multiplicative notation to the path composition operator.\n","permalink":"https://MikeLilley.github.io/posts/simplicial_homology/","summary":"We discuss the problem of hole-counting within topological spaces via triangulation with simplicial complexes.","title":"Simplicial Homology"},{"content":" Introduction Much of modern algebraic geometry is dedicated to the classification of objects with respect to certain criterion. For instance, we may want to determine if a set of polynomial equations is consistent, i.e. if their common zero set is nonempty. The ideal membership problem is one such problem of classification, which states the following: Given some polynomial \\(f \\) and an ideal \\(I \\in \\mathbb{K}[x_1, \\cdots, x_n],\\) is \\(f \\in I \\)?\nLike many things in mathematics, while capable of being stated simply, this in general is anything but an easy problem. Despite this, its immense importance and widespread applicability to problems inside and outside of algebraic geometry has encouraged research into finding solutions. The goal of the following section is to provide motivation and make the reader knowledgeable of computational solutions for the ideal membership problem. We will begin by exploring polynomials of one variable, then move onto the multivariate case.\nPolynomials of One Variable Let \\(\\mathbb{K} \\) be an arbitrary field and let \\(f \\in \\mathbb{K}[x] \\) be the polynomial \\(f = \\sum_{i=0}^n c_ix^i \\) where \\(c_i \\in \\mathbb{K} \\) and \\(c_n \\neq 0.\\) As a reminder, a polynomial ideal is a subset \\(I \\subseteq \\mathbb{K}[x]\\) such that elements in \\(I\\) are closed under multiplication with elements from the polynomial ring \\(\\mathbb{K}[x]\\) and under addition from elements also in \\(I.\\) That is to say, a polynomial ideal is just an ideal with its elements coming from \\(\\mathbb{K}[x].\\)\nAs stated in the following proposition, we can draw a parallel between vector spaces and ideals in the sense that an ideal can be thought of as the set of all linear combinations between products in our ring \\(\\mathbb{K}[x]\\) and some basis set \\(B \\subseteq I .\\) In the case of one variable, this basis set is a singleton set.\nProposition 2.1: If \\(\\mathbb{K}\\) is a field, then every ideal \\(I\\) of \\(\\mathbb{K}[x]\\) can be expressed as \\(\\langle g \\rangle = \\sum_i f_i g\\) for all \\(f_i \\in \\mathbb{K}[x]\\) and some \\(g \\in I.\\)\nThe proof for this requires Euclid's Division Lemma as adapted to polynomials, which we will state without proof as follows:\nLemma 2.2: Let \\(\\mathbb{K}\\) be a field and let \\(g \\in \\mathbb{K}[x]\\) be a nonzero polynomial. Then, every \\(f \\in \\mathbb{K}[x]\\) can be written as \\(f = qg + r,\\) where \\(q,r \\in \\mathbb{K}[x]\\) are unique and either \\(r = 0\\) or \\(deg(r) \u003c deg(g).\\) Furthermore, we may find \\(q\\) and \\(r\\) by means of an algorithm.\nNow, we may prove Proposition 2.1.\nProof. Let \\(I \\subseteq \\mathbb{K}[x]\\) be an ideal. If we consider the trivial case of \\(I = \\lbrace 0 \\rbrace,\\) we know that this ideal is generated by \\(\\langle 0 \\rangle.\\) For the non-trivial case, let \\(g\\) be a nonzero polynomial of minimum of degree contained in \\(I.\\) Our claim is that \\(\\langle g \\rangle = I.\\) We will proceed to prove this by mutual set inclusion. \\(\\langle g \\rangle \\subseteq I\\) is trivial as \\(I\\) contains all polynomials of the form \\(\\langle g \\rangle = \\sum_i f_i g\\) since \\(g \\in I.\\) As such, we will prove \\(I \\subseteq \\langle g \\rangle.\\) Consider an arbitrary polynomial \\(f \\in I.\\) By Lemma 2.2, we have that \\(f = qg + r.\\) Since \\(I\\) is an ideal, we know that \\(qg \\in I,\\) which implies that \\(r = f - qg \\in I,\\) since ideals are closed under addition. Our division lemma states that either \\(r = 0\\) or \\(deg(r) \u003c deg(g).\\) If \\(r \\neq 0,\\) then a contradiction is produced, as this implies that all polynomials in \\(I\\) are of a larger degree than \\(r,\\) thus \\(r \\not \\in I.\\) As a result, \\(r = 0.\\) This means that \\(f = qg,\\) therefore \\(f \\in \\langle g \\rangle.\\) \u0026#9724;\nThis characterization is useful since it allows us to talk about whether a polynomial is an element of some ideal if it can be described as a linear combination of \\(g\\) with elements of \\(\\mathbb{K}[x].\\) In general, we call \\(g\\) a generating polynomial or generator. However, we have not yet stated how we can find such a generator. Recall that if \\(f\\) can be written as a linear combination of \\(g,\\) then \\(f\\) is a multiple of \\(g:\\)\n$$f = q_1 g + q_2 g + \\cdots + q_k g = g(q_1 + q_2 + \\cdots + q_k) $$ Thus, we know that \\(g\\) should always divide \\(f.\\) As it turns out, the greatest common divisor of the polynomials \\(f_1, \\cdots, f_s\\) will be the generator of \\(\\langle f_1, \\cdots, f_s \\rangle.\\) We assume the reader is familiar with polynomial division, so we do not spend time on introducing things such as the existence and uniqueness property of the greatest common divisor or the Euclidean Algorithm.\nTheorem 2.3: Let \\(f_1, \\cdots, f_s \\in \\mathbb{K}[x].\\) Then, \\(\\gcd(f_1, \\cdots, f_s)\\) is a generator for the ideal \\(\\langle f_1, \\cdots, f_s \\rangle.\\)\nProof: Our goal is to show that the generator \\(d\\) for the ideal \\(\\langle f_1, \\cdots, f_s \\rangle\\) is equivalent to the greatest common divisor \\(\\gcd(f_1, \\cdots, f_s)\\) by proving that it satisfies the following two properties: \\(d\\) divides \\(f_1, \\cdots, f_s.\\)\nIf \\(p\\) is another polynomial which divides \\(f_1, \\cdots, f_s,\\) then \\(p\\) divides \\(d.\\)\nBy doing this, we demonstrate that it may be computed by the Euclidean algorithm. Consider the ideal \\(\\langle f_1, \\cdots, f_s \\rangle.\\) By Proposition 2.1, we know that \\(\\exists g \\in \\langle f_1, \\cdots, f_s \\rangle\\) such that \\(g\\) is a generator for this ideal. (1) is satisfied easily, since we may write each \\(f_i\\) as a multiple of \\(g\\) with an element of \\(\\mathbb{K}[x]\\) as we did above. To proceed with (2), suppose that there exists a polynomial \\(p \\in \\mathbb{K}[x]\\) that divides each \\(f_i.\\) This means that each \\(f_i = q_i p\\) for some \\(q_i \\in \\mathbb{K}[x].\\) Since \\(g \\in \\langle f_1, \\cdots, f_s \\rangle,\\) we know that there exists some \\(k_1, \\cdots k_s\\) such that \\(k_1 f_1 + \\cdots + k_s f_s = g.\\) If we substitute, we have the following:\n$$g = k_1 f_1 + \\cdots + k_s f_s = k_1 (q_1p) + \\cdots + k_s (q_sp) = (q_1 + \\cdots + q_s)p $$\nThis shows that \\(p\\) divides \\(g.\\) Hence, \\(g = \\gcd(f_1, \\cdots, f_s).\\) \u0026#9724;\nWe utilize the fact that \\(\\gcd(f_1, \\cdots, f_s) = \\gcd(f_1, \\gcd(f_2, \\cdots, f_s))\\) in order to construct our generator explicitly, so we can find the greatest common divisor of many polynomials by recursively computing it for two polynomials in the ideal. As a demonstration of how this works, consider the following example:\nExample 2.4: Suppose we want to find the generator of the ideal given by: $$I = \\langle x^4 - 1, x^6 - 1, x^3 - x^2 + x - 1 \\rangle $$ If we order by degree and use Euclidean division, we have the following:\n$$\\gcd(x^6 - 1, x^4 - 1) = \\gcd(x^4 - 1, x^2 - 1) = \\gcd(x^2 - 1, 0) = x^2 - 1 $$\n$$\\gcd(x^2 - 1, x^3 - x^2 + x - 1) = x - 1 $$\nTherefore, \\(g = x - 1\\) and \\(\\langle x^4 - 1, x^6 - 1, x^3 - x^2 + x - 1 \\rangle = \\langle x - 1 \\rangle.\\)\nWe can now apply our construction to our original problem, that of ideal membership. Consider some ideal \\(I = \\langle f_1, \\cdots, f_s \\rangle.\\) By Proposition 2.1, \\(\\langle f_1, \\cdots, f_s \\rangle = \\langle g \\rangle\\) for some \\(g \\in I,\\) and we can find exactly \\(g\\) is by means of the Euclidean algorithm. By Lemma 2.2, we know that \\(f = qg+r.\\) An observation we should make is that \\(f \\in I\\) if and only if \\(r = 0,\\) since we assume that \\(deg(r) \u003c deg(g),\\) so a representation of \\(f\\) which requires a polynomial of lower degree would not be in the ideal, as \\(r\\) cannot be written as a linear combination of \\(g.\\) This gives us a test to determine if a polynomial is contained within an ideal.\nCorollary 2.5: Given some polynomial \\(f\\) and an ideal \\(I,\\) \\(f\\in I\\) if and only if the generator of \\(I\\) divides \\(f.\\)\nProof: We know from Theorem 2.3 that a generator \\(g\\) of the ideal \\(I = \\langle f_1 \\cdots, f_n \\rangle\\) is \\(\\gcd(f_1 \\cdots, f_n).\\) In addition, we know that if \\(f \\in I,\\) then any generator \\(g\\) of \\(I\\) should divide \\(f.\\) Chaining these facts together proves our corollary.\nWe illustrate the use of this theorem with an example.\nExample 2.6: Consider our ideal in Example 1.4.\n\\(x^7 + 1 \\not \\in I,\\) as performing polynomial long division with \\(x - 1\\) gives us a remainder of \\(2/(x-1).\\)\n\\(x^{10} - x^6 - x^4 + 1 \\in I,\\) since we have no remainder following polynomial long division.\nRemark 2.7: This applies to any field, even those with nonzero characteristic.\nPolynomials of Many Variables To recap, we have explored the use of the Euclidean algorithm to retrieve a generator for an ideal \\(I \\in \\mathbb{K}[x],\\) and used it to determine if a polynomial is an element of \\(I.\\) In this section, our goal is to generalize this process to an arbitrarily finite amount of variables. This comes with several challenges that we do not face within the context of one variable, and we'll find that additional algebraic machinery is needed in order to do so without pitfalls. We'll begin this section by attempting to recreate the Euclidean division algorithm for polynomials of multiple variables.\nProposition 3.1: Consider taking polynomials \\(f_1,f_2 \\in \\mathbb{K}[x_1,\\cdots, x_n]\\) and dividing \\(f_1\\) by \\(f_2.\\) We claim that there exists an algorithm to do so.\nBefore we explore this proposition, we must introduce some terminology which will be important going forward:\nDefinition 3.2: The multidegree of a polynomial \\(f\\) is given as: $$\\mbox{multideg}(f) = \\max(\\alpha \\in \\mathbb{N}^n \\mid a_\\alpha \\neq 0) $$ The leading coefficient of a polynomial \\(f\\) is given as:\n$$\\mbox{LC}(f) = a_{\\mbox{multideg}(f)} \\in \\mathbb{K}$$ The leading monomial of a polynomial \\(f\\) is given as:\n$$\\mbox{LM}(f) = x^{\\mbox{multideg}(f)}$$ The leading term of a polynomial \\(f\\) is given as:\n$$\\mbox{LT}(f) = \\mbox{LM}(f) \\cdot \\mbox{LC}(f)$$ Example 3.3: Consider the polynomial \\(f = 5xy^6 + 2z:\\)\n\\(\\mbox{multideg}(f) = (1,6,0) \\)\n\\(\\mbox{LC(f)} = 5 \\)\n\\(\\mbox{LM(f)} = xy^6 \\)\n\\(\\mbox{LT(f)} = 5xy^6 \\)\nAn important point to notice is that tie-breaking conditions for these functions are not yet well-defined. If instead \\(f = 5xy^6 + 2z^7,\\) does \\(\\mbox{LT(f)}\\) give us \\(5xy^6\\) or \\(2z^7\\)? In the following few paragraphs, we will be taking a detour to describe objects called orderings which we associate with polynomials in order to break ties.\nIn the single-variable case, defining the Euclidean algorithm and speaking of things like the leading term is simple, as we just use the standard total order \\(\u003c\\) on the natural numbers with respect to the powers of our variable. In more than one variable, the use of this ordering breaks down. As such, we need to define a new ordering. This brings us to our first definition:\nDefinition 3.4: A monomial ordering \\(\u003e\\) on \\(\\mathbb{K}[x_1, \\cdots, x_n]\\) is a relation \\(\u003e\\) on \\(\\mathbb{Z}^n_{\\geq 0}\\) satisfying the conditions that \\(\u003e\\) is a total and well ordering on \\(\\mathbb{Z}^n_{\\geq 0}\\) and \\(\u003e\\) is preserved under addition.\nWith our \\(\\mbox{LT}\\) function listed above now being well-defined, we proceed with our original goal of constructing a division algorithm for multivariate polynomials for Proposition 3.1. In addition to generalizing the algorithm to polynomials of multiple variables, we will also generalize to support multiple divisors. The reason for this is as follows: even in the single variable case, we may want to divide a polynomial by many divisors in an effort to factor it. Consider the following polynomial:\n$$f(x) = (x-1)(x + 1)^2 = x^3 + x^2 - x + 1$$\nIf we were to divide this polynomial with the divisors \\(x + 1\\) and \\(x - 1,\\) we would have to perform the division algorithm twice, which is computationally expensive as opposed to just dealing with multiple divisors in one pass of the algorithm. Doing this in multiple variables might be somewhat unintuitive at first; we will define the algorithm, go through some examples, then describe caveats which arise when compared to the single-variable case.\nLet's now go through an example to demonstrate this algorithm being applied.\nExample 3.7: Let \\(f = x^2y + xy^2 + y^2\\) and let our divisors be \\(f_1 = xy - 1\\) and \\(f_2 = y^2 - 1.\\) Since we label \\(xy-1\\) with \\(f_1,\\) we will denote that as our first divisor.\nWe saw in the single-variable case that the Euclidean algorithm gave us a generator \\(g\\) for our ideal, and we were able to test if a polynomial \\(f\\) was a member of an ideal or not by dividing \\(f\\) by \\(g\\) and seeing if \\(f/g\\) had a remainder. Unfortunately, this test is not as straightforward with many variables. We note that if after following division, we find that \\(f = q_1 g_1 + \\cdots + q_s g_s,\\) then obviously \\(f \\in \\langle f_1, \\cdots, f_s \\rangle.\\) However, as we’ll see next, this is not a requirement. Consider computing division as we do above with the divisors in a different order.\nExample 3.8: Let \\(f = xy^2 -x\\) and let our divisors be \\(f_1 = xy - 1\\) and \\(f_2 = y^2 - 1.\\) If we divide by \\(f_1\\) first, we get \\(xy^2 - x = y(xy-1) - x + y.\\) However, if we divide by \\(f_2\\) first, we have \\(xy^2 - x = x(y^2 - 1).\\)\nHence, we see that the test that we constructed in Corollary 2.5 does not carry over as readily to multivariate polynomials as well as we would hope. With these issues arising from our attempt to translate the polynomial division algorithm into multiple variables, one might expect we need a different kind of test to determine ideal membership in \\(\\mathbb{K}[x_1,\\cdots, x_n].\\) As it turns out, this method is completely fine; the issues we see are a result of our choice of divisors \\(f_1, \\cdots, f_s.\\) In this next section, we will explore how translating the defining polynomials \\(f_1, \\cdots, f_s\\) of an ideal into a different set \\(g_1, \\cdots, g_s\\) which has nicer properties but still defines the same ideal will produce desired behavior, where remainders are unique and a polynomial \\(f\\) is in an ideal if and only if the remainder is zero following polynomial division.\nGröbner Bases In this section, we will be constructing the bases with desirable properties we described in the end of the last section. We begin with three definitions and a lemma which we state without proof.\nDefinition 4.1: Let \\(I \\subseteq \\mathbb{K}[x_1,\\cdots, x_n]\\) be a nontrivial ideal and fix a monomial ordering on \\(\\mathbb{K}[x_1,\\cdots, x_n].\\) We define \\(\\mbox{LT}(I)\\) as the set of leading terms of nonzero elements in \\(I\\) given by:\n$$\\mbox{LT}(I) = \\lbrace cx^a \\mid \\exists f\\in I \\setminus \\lbrace 0 \\rbrace \\mbox{ with } \\mbox{LT}(f) = cx^a \\rbrace.$$\nDefinition 4.2: Let \\(I\\) be an ideal generated by a set of monomials in \\(\\mathbb{K}[x_1, \\cdots, x_n].\\) Then, we call \\(I\\) a monomial ideal.\nLemma 4.3: Let \\(I\\) be a monomial ideal. Then, the monomial \\(\\alpha = x_{1}^{a_1} \\cdots x_{n}^{a_n}\\) is a member of \\(I\\) if and only if there exists a monomial \\(\\beta \\in I\\) which divides \\(\\alpha.\\)\nDefinition 4.4: We say that \\(\\langle \\mbox{LT}(I) \\rangle\\) is the ideal generated by elements of \\(\\mbox{LT}(I).\\)\nIt is important to note that \\(\\langle \\mbox{LT}(I) \\rangle\\) and the ideal given by the leading terms of its defining polynomials \\(\\langle \\mbox{LT}(f_1), \\cdots \\mbox{LT}(f_s), \\rangle\\) are not necessarily the same ideal. In particular, it is often that \\(\\langle \\mbox{LT}(f_1), \\cdots \\mbox{LT}(f_s) \\rangle \\subseteq \\langle \\mbox{LT}(I) \\rangle.\\) The converse is never the case, as \\(\\mbox{LT}(f_i) \\in \\mbox{LT}(I)\\) as \\(f_i \\in I.\\) Consider the following example:\nExample 4.5: Let \\(I = \\langle x^3 - 2xy, x^2y-2y^2+x \\rangle.\\) If we use the \\(grlex\\) ordering, our leading terms are \\(x^3\\) and \\(x^2y.\\) We know that we can create \\(x^2\\) from the following linear combination of our two functions as follows:\n$$x(x^2y-2y^2+x) - y(x^3 - 2xy) = x^2 $$\nHowever, we can’t divide \\(x^2\\) by \\(x^3\\) or \\(x^2y,\\) so we know it does not exist in the ideal \\(\\langle x^3, x^2y \\rangle\\) by Lemma 4.3.\nThe reader might ask if there is anything special about the bases which have the property where \\(\\langle LT(f_1), \\cdots LT(f_s) \\rangle = \\langle LT(I) \\rangle.\\) Indeed, there very much is. Before we discuss these though, we will need to introduce a theorem without proof.\nTheorem 4.6 (Hilbert’s Basis Theorem): Every ideal \\(I \\in \\mathbb{K}[x_1, \\cdots, x_n]\\) is Noetherian, or has a finite generating set \\(g_1, \\cdots, g_t \\in I.\\)\nThis is a powerful result, since we know we can represent any ideal completely with a finite amount of space. We now exhibit an important definition which describes a particular type of finite generating set.\nDefinition 4.7: If \\(g_1,\\cdots,g_t \\in I\\) and \\(\\langle LT(I) \\rangle = \\langle LT(g_1), \\cdots, LT(g_t) \\rangle,\\) then we call \\(g_1,\\cdots,g_t\\) a Gröbner Basis to \\(I.\\)\nRemark 4.8: Every ideal \\(I\\) has a Gröbner Basis.\nThese bases are incredibly important for a variety of reasons. In our case, they will solve our problem of non-uniqueness in the division algorithm for \\(\\mathbb{K}[x_1,\\cdots, x_n],\\) as illustrated by the following proposition and corollary:\nProposition 4.9: Let \\(I \\subseteq \\mathbb{K}[x_1,\\cdots, x_n]\\) be an ideal and let \\(G = \\lbrace g_1,\\cdots, g_t \\rbrace\\) be a Gröbner Basis for \\(I.\\) Then, given \\(f \\in \\mathbb{K}[x_1,\\cdots, x_n],\\) there exists a unique \\(r\\) with the following two properties:\nNo term of \\(r\\) is divisible by any of the terms \\(LT(g_1), \\cdots, LT(g_t).\\)\nThere is a \\(g \\in I\\) such that \\(f = g + r.\\)\nProof: Given \\(f,\\) we know that our division algorithm can produce \\(f = \\sum_i q_ig_i + r,\\) where \\(r\\) satisfies \\((1).\\) If we let \\(g = \\sum_i q_ig_i \\in I,\\) then \\((2)\\) is also satisfied. To prove that our \\(r\\) is unique, suppose that \\(f = g+r = g' + r'.\\) Since \\(r\\) satisfies our two conditions, we know that \\(r - r' = g - g' \\in I.\\) This implies that if \\(LT(r - r') \\in \\langle I \\rangle = \\langle LT(g_1),\\cdots,LT(g_t) \\rangle,\\) then \\(LT(r - r')\\) is divisible by some \\(LT(g_i).\\) This is because a monomial \\(x_{1}^{a_1} \\cdots x_{n}^{a_n}\\) exists in a monomial ideal if and only if it is divisible by some monomial in the ideal as described by Lemma 4.3. However, this is impossible, as \\(deg(r) \u0026lt; deg(g_i)\\) if \\(r \\neq 0\\) which means \\(r \\neq 0.\\) Thus, \\(r = 0\\) and \\(r = r',\\) which proves that \\(r\\) is unique.\nFinally, we may construct the following corollary:\nCorollary 4.10: Let \\(I \\subseteq \\mathbb{K}[x_1,\\cdots, x_n]\\) be an ideal and let \\(G = \\lbrace g_1,\\cdots, g_t \\rbrace\\) be a Gröbner Basis for \\(I.\\) Then, \\(f \\in I\\) if and only if the remainder of \\(f\\) after division with \\(G\\) is 0.\nProof: If \\(r = 0,\\) then \\(f = \\sum_i q_ig_i,\\) which implies that \\(f \\in I.\\) For the converse, if \\(f \\in I,\\) then \\(f = f + 0\\) satisfies the two conditions above, which implies that 0 is the remainder of \\(f\\) after dividing by the elements in our Gröbner Basis \\(G.\\)\nAt this point, one might wonder how we can generate a Gröbner Basis explicitly. There are many algorithms to do this, and I ask the reader to refer to [1] to see the ones typically used in modern computer algebra systems such as the F5 Signature algorithm, or for Buchberger’s classic algorithm.\nWith Corollary 4.10, we have solved our dilemma regarding non-unique remainders when performing polynomial division in multiple variables, and subsequently determining ideal membership in \\(\\mathbb{K}[x_1, \\cdots, x_n].\\) In the next section, we will show how we can use the ideal membership test to solve constraint satisfaction problems.\nConstraint Satisfaction Problems Up to this point, we have spent time developing a theory of Gröbner Bases in an effort to solve the Ideal Membership Problem in many variables. While a very rich theory in of itself, we have progressed enough in our algebraic geometry prerequisites to now turn our attention to defining CSPs and showing how it can be determined if a CSP is satisfiable using the tools we now have.\nOn an intuitive level, a constraint satisfaction problem is a problem where one must choose a solution from a set of possible solutions which adheres to a given set of constraints. These may be hard constraints (a necessary set of conditions) or constraints of preference (a set of conditions which are nice to have but not required). For the sake of simplicity, we will only be considering hard constraints and finite amounts of variables. Common examples of CSPs include the integer programming problem or the graph coloring problem, the latter of which we will be using as an example.\nIn this section, our goal is to develop a method to translate a CSP into a corresponding ideal membership problem.\nDefinition 5.1: A Constraint Satisfaction Problem is a triple \\((X, D, C)\\) where \\(X\\) is an \\(n\\)-tuple \\((x_1, \\cdots, x_n)\\) of variables, \\(D\\) is an indexed collection of \\(n\\) sets where each set \\(D_i\\) is the domain of the \\(i^{th}\\) variable in \\(X,\\) and \\(C\\) is a set of pairs \\((t_j, C_j),\\) where \\(t_j\\) is a set of variables \\(t_j \\subseteq X\\) and \\(C_j\\) is a relation (a subset of the Cartesian product) on the domains of the variables in \\(t_j.\\) Our set of possible solutions \\(P\\) is defined as the \\(n\\)-ary Cartesian product amongst the domains of all variables in \\(X.\\) Our set of solutions \\(S \\subseteq P\\) is the set of \\(n\\)-tuples which satisfies every constraint in \\(C.\\) If \\(S = \\varnothing,\\) we say that \\((X, D, C)\\) is unsatisfiable. To be more explicit about the constraints, we say that: $$ C_j \\subseteq \\bigtimes_{x \\in {t_j}} D_x $$\nas the \\(k\\)-ary Cartesian product amongst all \\(k\\) variables in \\(t_j\\) represents all possible configurations between values they could take on from their domains, and a constraint limits the values we can choose to a subset of this Cartesian product. To illustrate, consider the following toy example:\nExample 5.2: Let \\(X = \\lbrace x, y \\rbrace,\\) \\(D = \\lbrace \\mathbb{Z}, \\mathbb{Z} \\rbrace\\) and let our only constraint be that the sum of \\(x\\) and \\(y\\) be positive. Thus, our constraint is:\n$$C_j = \\lbrace (x,y) \\in \\mathbb{Z}^2 \\mid x + y \u0026gt; 0 \\rbrace \\subseteq \\mathbb{Z} \\times \\mathbb{Z}$$\nNext, we will describe graph coloring as a constraint satisfaction problem.\nDefinition 5.3: A \\(G = (V,E)\\) is a collection of two sets; a set of vertices \\(V,\\) and a set of edges \\(E,\\) which consists of ordered pairs \\((x_i,y_i)\\) where \\(x_i,y_i \\in V.\\)\nDefinition 5.4: We may state the as follows: A graph \\(G\\) is \\(k\\)-colorable if there exists a set of distinct colors \\(A\\) such that \\(|A| = k\\) and there exists a function \\(f\\) to assign a color to each vertex \\(v \\in V \\in G\\) such that no two vertices connected by an edge \\(e \\in E \\in G\\) have the same color.\nFinding efficient ways to solve the graph coloring problem is significant because many different problems can be reformulated as a graph coloring problem. This includes various scheduling problems, register allocation, and pattern matching.\nWe may write the graph coloring problem as a CSP as follows: Let \\((X, D, C)\\) be our graph coloring problem in CSP formulation and let \\(A\\) be our set of colors. We set \\(X = V,\\) \\(D = \\lbrace A_1, \\cdots, A_n \\rbrace,\\) and \\(C = \\lbrace x_i \\neq x_j, \\forall v_i,v_j \\in e \\in E \\in G \\rbrace.\\)\nOur set of possible solutions is given by: $$C_j = \\bigtimes_{1 \\leq i \\leq n} D_x$$\nAs this represents all possible color configurations between \\(n\\) variables. Our goal is to find our solution set \\(S \\subseteq P.\\)\nWe can attack this problem using ideal membership, as shown in [9]. To do this, we will work in the ring \\(\\mathbb{C}[x_1, \\cdots, x_n],\\) where each of our \\(k\\) colors are assigned to the \\(k\\) roots of unity. We will now define the following polynomial which gives us information about the graph:\nDefinition 5.5: The \\(f_G \\in \\mathbb{C}[x_1, \\cdots, x_n]\\) associated with the graph \\(G\\) is given by:\n$$f_G = \\prod_{v_i, v_j \\in e} (v_i - v_j)$$\nNotice that if we assign two vertices \\(v_i\\) and \\(v_j\\) connected by an edge the same color (root of unity), then \\(f_G\\) vanishes. We now have the following theorem which relates the graph polynomial to the ideal membership problem. To prove this, we need to invoke a famous theorem in algebraic geometry, Hilbert’s Nullstellensatz, or translated, Hilbert’s Theorem of Zeroes. This is actually one of three forms of the Nullstellensatz; they are each given the adjectives weak, standard and strong, since you need the each prior form to prove the next. Due to the amount of labor involved in proving any of the Nullstellensatz theorems, we will state the standard version without proof.\nTheorem 5.6 (Hilbert's Nullstellensatz): Let \\(\\mathbb{K}\\) be an algebraically closed field. If \\(f, f_1, \\cdots f_s \\in \\mathbb{K}[x_1,\\cdots,x_n],\\) then \\(f \\in \\mathbb{I}(\\mathbb{V}(f_1, \\cdots, f_s))\\) if and only if: $$ f^m \\in \\langle f_1, \\cdots, f_s \\rangle $$ for some \\(m \\in \\mathbb{N}.\\)\nWe may now state our theorem.\nTheorem 5.7: Let \\(k \\in \\mathbb{N}\\) and let the graph \\(G\\) have \\(n\\) nodes. In addition, let \\(I = \\langle v_1^k - 1, \\cdots, v_n^k - 1 \\rangle.\\) The graph \\(G\\) is \\(k\\)-colorable if and only if \\(f_G \\not \\in I.\\)\nProof. \\((\\implies):\\) If \\(G\\) is \\(k\\)-colorable and let \\(G\\) have \\(n\\) nodes. We may assign a color to each vertex such that no vertex sharing an edge has the same color. Denote each assigned color as \\(a_1, \\cdots, a_n.\\) Since our graph polynomial only vanishes when two adjacent vertices are the same color, we know that \\(f_G(a_1, \\cdots, a_n) \\neq 0.\\) Since we assigned our colors to the \\(k\\) roots of unity, we know that \\((a_1, \\cdots, a_n) \\in \\mathbb{V}(I).\\) Since any polynomial in \\(I\\) must vanish at points within \\(\\mathbb{V}(I),\\) we know that \\(f_G \\not \\in I.\\) \\((\\impliedby):\\) Let \\(v_i^k - 1 = f_i\\) and suppose \\(G\\) is not \\(k\\)-colorable. Then, all possible assignments of colors will make \\(f_G\\) vanish, which means that \\(f_G\\) vanishes on all points in the variety \\(\\mathbb{V}(I).\\) This means that \\(f_G \\in \\mathbb{I}(\\mathbb{V}(f_1, \\cdots, f_n)),\\) which by the Nullstellensatz implies that \\(f^m \\in I.\\) \\(I\\) is a radical ideal, which means that \\(f \\in I.\\) \u0026#9724;\nWe can now illustrate the use of this theorem with an example.\nExample 5.9: Consider the complete graph \\(G = K_3,\\) given by the triangle of three vertices all mutually connected to one another:\nWe may define our graph polynomial as \\(f_G = (x - z)(y - z)(x - y).\\) If we choose to 3-color this graph, we may define our ideal as \\(I_3 = \\langle x^3 - 1, y^3 - 1, z^3 - 1 \\rangle.\\) None of the defining polynomials in \\(I\\) divide \\(f_G\\) which implies that \\(f_G \\not \\in I_3\\) so we may say that this graph is 3-colorable. If instead we try to 2-color this graph, our ideal is \\(I_2 = \\langle x^2 - 1, y^2 - 1, z^2 - 1 \\rangle.\\) \\(x^2 - 1\\) divides our graph polynomial, so this implies that \\(f_G \\in I_2,\\) and thus cannot be 2-colored.\nSo, we just showed how the graph coloring problem could be framed as an ideal membership problem in the sense that the answer to if a particular graph \\(G\\) could be \\(k\\)-colored was determined based on if its graph polynomial \\(f_G\\) was a member of a particular ideal. However, we did not describe why we decided to frame the problem this way, or if there’s a more general framework to translate CSPs into IMPs. As it turns out, there is a method we can use to translate any CSP into an equivalent IMP, as described in [3]. Let our CSP be given as \\((X, D, C)\\) where \\(|X| = n.\\) To begin, we assume \\(D \\subseteq \\mathbb{K}^n\\) for some field \\(\\mathbb{K}.\\) At first glance, it might seem like we lose some generality here, as there is no requirement in the definition of our CSP that our variables need to take on the same domains, or even be a field for that matter. For instance, \\(x_1\\) could be integer-valued and \\(x_2\\) could be real-valued. There is no such loss here, as we can simply pick a field which is large enough to contain the domains of all of our variables following an injective mapping. In the case above, we might decide to map \\(x_1\\) to \\(\\mathbb{R},\\) or we might also decide to map \\(x_1\\) and \\(x_2\\) to \\(\\mathbb{C},\\) so we can take advantage of nice properties such as algebraic closure. Our goal in this correspondence is to describe our solution set \\(S\\) as the vanishing set of some ideal \\(I \\subseteq \\mathbb{K}[x_1,\\cdots, x_n].\\) In doing so, we map each constraint to a generating set of some ideal. Consider an arbitrary constraint \\((t_j, C_j)\\) where \\(|t_j| = k.\\) As described above, \\(C_j\\) is a subset of the \\(k\\)-ary Cartesian product and consists of points of the form \\((v_1,\\cdots,v_k).\\) Let \\(x_{i_a} \\in t_j.\\) Consider the ideal of \\(\\mathbb{K}[x_1,\\cdots, x_n]\\) for some point \\(v \\in C_j\\) given by \\(I_{j_v} = \\langle x_{i_1} - v_1, \\cdots,x_{i_k} - v_k \\rangle\\); we know this is maximal and therefore radical since it is an ideal of linear polynomials. Likewise, we know that \\(\\mathbb{V}(I_{j_v}) \\in C_j \\subseteq \\mathbb{K}^k.\\) As a standard result about varieties, we know that \\(\\mathbb{V}(I \\cap J) = \\mathbb{V}(I) \\cup \\mathbb{V}(J).\\) Also, it is a property of radical ideals that \\(\\mathbb{V}(\\mathbb{I}(V)) = V.\\) As such, we can say that: $$C_j = \\bigcup_{v \\in C_j} \\mathbb{V}(\\langle x_{i_1} - v_1, \\cdots,x_{i_k} - v_k \\rangle) = \\mathbb{V}(\\mathbb{I}(C_j)) \\quad \\mbox{where} \\quad \\mathbb{I}(C_j) = \\bigcap_{v \\in C_j} I_{j_v}$$ We know that \\(\\mathbb{I}(C_j)\\) is zero-dimensional since it is finite and radical since the intersection of radical ideals is radical. If we wanted to construct a set of points which satisfies all constraints in the CSP, a natural idea is to just simply take the intersection of the vanishing sets of all \\(C_j \\in C.\\) By the above lemma, we know that the intersection of the vanishing sets of many ideals is equivalent to the vanishing set of the sum of all the ideals. Since \\(\\mathbb{V}(I) \\cap \\mathbb{V}(J) = \\mathbb{V}(I + J),\\) this may be written as:\n$$\\bigcap_{C_j \\in C} \\mathbb{V} \\Bigg ( \\bigcap_{v \\in C_j} I_{j_v} \\Bigg ) = \\mathbb{V} \\Bigg ( \\sum_{C_j \\in C} \\bigcap_{v \\in C_j} I_{j_v} \\Bigg )$$ The last thing we need to consider is how to ensure that the vanishing set of our ideal is consistent with the domains of each variable when we operate in a field larger than some domain \\(D_i.\\) For this, we introduce a notion of a .\nConsider the ideal \\(\\mathbb{I}(C_j)\\) given above. We will take the intersection of this ideal with the ideal that has \\(|t_j| = k\\) generators, and the generator corresponding to a particular variable \\(\\gamma = x_{i_a} \\in t_j\\) given by: $$g_\\gamma = \\prod_{p \\in D_\\gamma} (\\gamma - p)$$\nNow, if we redefine our ideal as \\( C_j’ = C_j \\cap \\langle g_{\\gamma_1}, \\cdots, g_{\\gamma_k} \\rangle ,\\) we know that \\(\\mathbb{I}(C_j)'\\) will vanish if and only if each variable takes on a value in their respective domain. We will see the necessity of these domain polynomials in Example 6.2.\nFinally, note that while the sum of radical ideals is not necessarily radical, we invoke the following theorem from [2] to show that in our particular case, we know that the ideal given by the sum of our ideals is radical.\nTheorem 5.10: Let \\(m \\in \\mathbb{N}.\\) For each \\(i \\leq m,\\) let \\(X_i\\) be a non-empty set of variables. Furthermore, let \\(X=\\bigcup _{i=1}^mX_i,\\) let \\(R=\\mathbb{K}[X],\\) let \\(I_i\\) be a zero-dimensional radical ideal of \\(k[X_i]\\) and let \\(R_i\\) be the \\(R\\)-module of \\(I_i\\) for \\(1 \\leq i \\leq m.\\) Finally, let \\(J \\subseteq \\mathbb{K}[X]\\) be the ideal given by \\(J = \\sum_{i=1}^m R_i.\\) Then, \\(J\\) is either inconsistent or a zero-dimensional radical ideal of \\(R.\\)\nWe ask the reader to refer to Proposition 3.22 of [2] for the proof of this theorem. If our CSP has a solution, then this means that the sum of our ideals is radical. To find out if our CSP has a solution, we have to determine if our variety is empty. This is equivalent to determining if \\(1 \\in I.\\) It should be noted that once we compute the Gröbner basis, we no longer actually need to perform polynomial division to determine this. By Corollary 4.10, we know that if there are no constants in our Gröbner basis, then we obviously can’t have a zero remainder with a dividend of 1. The discussion in prior sections regarding the relation between polynomial division and ideal membership was primarily to motivate the construction of Gröbner bases. Now that we have them, we no longer need the algorithm described in the section Polynomials of Many Variables.\nFor the examples in the next section, we will use Singular [8] to compute Gröbner bases, which is a specialized computer algebra system for algebraic geometry.\nComputational Examples The Boolean Satisfiability Problem Our first problem we will solve will be a simple one, that of Boolean satisfiability. It states the following: Given some Boolean formula \\(F,\\) is there a configuration of its variables \\((x_1, \\cdots, x_n)\\) such that \\(F(x_1, \\cdots, x_n) = 1\\)? That is, can \\(F\\) be satisfied?\nConsider the formula given by \\(F = (A \\wedge B) \\vee (C \\wedge \\neg A) \\wedge (\\neg A \\vee \\neg B \\vee C).\\)\nThis is expressed as a CSP \\((X, D, C)\\) where \\(X = \\lbrace A, B, C \\rbrace\\) and \\(D = \\lbrace \\mathbb{Z}_2, \\mathbb{Z}_2, \\mathbb{Z}_2 \\rbrace.\\) To get our constraints, we will transform \\(F\\) into an equivalent expression that is in Disjunctive Normal Form (DNF). This form is a series of sub-expressions connected by or operators. That is to say, \\( F = \\bigvee_{i = 1}^k F_i .\\)\nAny Boolean expression can be transformed into DNF. The result following transformation is \\(F = (A \\wedge B) \\vee (\\neg A \\wedge C).\\) So, we know that the our constraints are given as:\n","permalink":"https://MikeLilley.github.io/posts/ideal-csp/","summary":"We describe the application of the ideal membership problem (IMP) and Gröbner bases to the problem of deciding constraint satisfaction problems (CSPs).","title":"Ideal Membership and Constraint Satisfaction Problems"},{"content":" Introduction There is a paper I studied as part of my undergraduate research titled Topic Modeling in Embedding Spaces by Dieng et al.; researchers based out of Columbia University. I particularly liked this work, and thought an article which goes over the content with additional explanation was appropriate for a post. In the following sections, we're going to be covering the prerequisites needed to understand the Embedded Topic Model, the ETM itself, as well as subsequent sections concerning variational inference using ELBO and the corresponding empirical study done to evaluate the model on real-world data.\nTopic Modeling We define a topic model as a statistical model meant to characterize documents in terms of their topics. Given only a collection of documents and the words in the documents, such a model would attempt to extract the latent topics and then use them to describe the documents. A topic is defined as a cluster of words which appear frequently together in documents. For help with an intuitive understanding, consider the following toy example:\nExample 1.1: Let our collection of documents be defined as \\(\\mathcal{D} = \\lbrace D_1,D_2,D_3 \\rbrace ,\\) each containing the following content:\n\\( D_1 = \\lbrace \\mbox{The quick brown fox jumped over the lazy dog.} \\rbrace \\) \\( D_2 = \\lbrace \\mbox{The dog and hare were distraught when the fox won the race.} \\rbrace \\) \\( D_3 = \\lbrace \\mbox{Baseball and racing are two of my favorite sports.} \\rbrace \\) Consider a very simple topic model \\(\\mathcal{M},\\) which performs part-of-speech tagging for each word in a document, extracts the nouns and performs lemmatization, clusters words based on mutual presence in a document, then maps each document to a vector \\(\\vec{v} \\in \\mathbb{R}^n\\) whose entries sum to 1, with \\(n\\) being the total number of distinct topics extracted from all documents. In this model, each element of the vector represents the proportion of nouns in the document which are assigned to the \\(i^{th}\\) topic represented by the value \\(\\vec{v}_i.\\) In our documents above, we might extract the set of topics \\(\\mathcal{T} = \\lbrace \\mbox{animals}, \\mbox{activities} \\rbrace\\) and assign words as follows:\n$$ \\begin{array}{|c|c|} \\hline \\textbf{Word} \u0026 \\textbf{Topic} \\\\ \\hline \\text{sports} \u0026 \\text{activities} \\\\ \\hline \\text{dog} \u0026 \\text{animals} \\\\ \\hline \\text{Baseball} \u0026 \\text{activities} \\\\ \\hline \\text{hare} \u0026 \\text{animals} \\\\ \\hline \\text{race} \u0026 \\text{activities} \\\\ \\hline \\end{array} $$ So, the documents would be mapped to the following vectors:\n$$ \\begin{array}{|c|c|} \\hline \\textbf{Document} \u0026 \\textbf{Vector} \\\\ \\hline D_1 \u0026 \\begin{bmatrix} 1.00 \u0026 0.00 \\end{bmatrix} \\\\ \\hline D_2 \u0026 \\begin{bmatrix} 0.75 \u0026 0.25 \\end{bmatrix} \\\\ \\hline D_3 \u0026 \\begin{bmatrix} 0.00 \u0026 1.00 \\end{bmatrix} \\\\ \\hline \\end{array} $$ Most topic models do not by themselves assign explicit semantic labels to the extracted latent topics, unlike in the above example. Generally, we need to assign them by hand or add additional structure that is capable of automatically generating meaningful labels.\nRelevant Probability Distributions The precursor to the Embedded Topic Model is a method called Latent Dirichlet Allocation, first described by Blei et al.; LDA is arguably still the most widely used topic model to date. Before proceeding, we need to understand a probability distribution which is central to both LDA and the embedded topic model called the Dirichlet distribution. The best way to understand the Dirichlet distribution is by means of the Beta distribution first, as the former is a multidimensional generalization of the latter.\nWe'll first examine the distributions by themselves, then describe why they are important based on the assumptions made concerning the structure of the documents within the topic models.\nThe Beta Distribution Definition 1.2: The Beta distribution is a probability distribution defined as: $$f(x; \\alpha; \\beta) = \\frac{1}{B(\\alpha, \\beta)} x^{\\alpha - 1}(1-x)^{\\beta - 1}$$ for \\(x ∈ [0, 1]\\) and \\(\\alpha , \\beta \u003e 0 \\)\n\\(1/B(\\alpha, \\beta)\\) is just a normalizing constant, so \\(f(x; \\alpha; \\beta) \\propto x^{\\alpha - 1}(1-x)^{\\beta - 1}.\\)\nRecall that the cumulative distribution function of any probability distribution must be 1 when evaluated at the upper limit of the sample space. This is because Kolmogorov's second axiom of probability states that since the sample space of a probability distribution represents the set of all possible outcomes of a random experiment, the probability of an experiment resulting in an outcome contained within the sample space is certain. Therefore, to retrieve the explicit normalizing constant, we can solve for \\(c \\) where:\n$$1 = c \\int_{0}^{1} x^{\\alpha - 1}(1-x)^{\\beta - 1}dx \\qquad \\longrightarrow \\qquad \\frac{1}{\\int_{0}^{1} x^{\\alpha - 1}(1-x)^{\\beta - 1}dx} = c $$ Our first step is to notice that our integral is very similar in form to the Laplace transform of the convolution of \\(x^{\\alpha - 1} \\) and \\(x^{\\beta - 1} :\\)\n$$\\int_{0}^{1} x^{\\alpha - 1}(1-x)^{\\beta - 1}dx \\qquad \\Bigg| \\qquad \\mathcal{L}(x^{\\alpha-1} * x^{\\beta-1}) = \\int_{0}^{t} \\tau^{\\alpha - 1}(1-\\tau)^{\\beta - 1}d \\tau$$ Using known Laplace transform identities, we can rewrite this as $$\\mathcal{L}(x^{\\alpha-1} * x^{\\beta-1}) = \\mathcal{L}(x^{\\alpha-1}) \\cdot \\mathcal{L}(x^{\\beta-1}) = \\frac{\\Gamma(\\alpha)}{s^\\alpha} \\cdot \\frac{\\Gamma(\\beta)}{s^\\beta} = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{s^{\\alpha+\\beta}}$$ where \\( \\Gamma(x) \\) is the gamma function. Since \\(\\Gamma(\\alpha)\\Gamma(\\beta)\\) is a constant, we can inverse transform this as:\n$$\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} t^{\\alpha+\\beta-1}$$ Since \\(t=1,\\) this gives us \\(\\Gamma(\\alpha)\\Gamma(\\beta)/\\Gamma(\\alpha+\\beta)\\) or the beta function. Thus,\n$$\\frac{1}{\\int_{0}^{1} x^{\\alpha - 1}(1-x)^{\\beta - 1} \\,dx} = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} = c$$ With this, we can now rewrite the beta distribution as:\n$$ f(x; \\alpha; \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} x^{\\alpha - 1}(1-x)^{\\beta - 1}$$ Tweaking the values of \\(\\alpha\\) and \\(\\beta\\) causes the beta distribution to take on different shapes. When both parameters are below 1 and equal, the graph resembles a U-shape, implying that the majority of the probability mass is present near the upper and lower limits of the sample space. When \\(\\alpha\\) is larger than \\(\\beta\\) or vice-versa, the majority of the mass is present near only one of the limits. When both limits are larger than 1, the mass is present towards the center of the sample space, resembling a bell curve.\nFigure 1.3: A Beta Distribution with Various \\(\\alpha\\) and \\(\\beta\\) Parameters\nThe Dirichlet Distribution We will now introduce the multivariate generalization of the beta distribution.\nDefinition 1.4: The Dirichlet distribution is a probability distribution defined as:\n$$f(x_1,\\cdots, x_k; \\alpha_1, \\cdots, \\alpha_k) = \\frac{1}{B(\\alpha)} \\prod_{i=1}^K x_i^{\\alpha_k - 1}$$ where \\(\\alpha = [\\alpha_1, \\cdots, \\alpha_k] ,\\) \\(\\sum_{i=0}^K x_i = 1,\\) and \\(\\alpha_k \u003e 0\\) for all \\(k.\\)\nNotice that if \\(K = 2\\) and \\(x_1 = 1 - x_2 ,\\) this reduces to the beta distribution. To account for more variables, we extend the definition of the beta function as follows:\n$$B(\\alpha) = \\frac{\\prod_{i=1}^K\\Gamma(\\alpha_i)}{\\Gamma(\\sum_{i=1}^K\\alpha_i)} $$ One of the many important aspects of the Dirichlet distribution is how it can be interpreted geometrically. The constraint \\(\\sum x_i = 1 \\) tells us that the support of the distribution is a regular simplex with \\(k \\) vertices. Recall that a regular simplex is the set of all points \\(p \\in \\mathbb{R}^k \\) such that \\(\\sum_{i=1}^k p_i = 1 .\\) To see this, consider the Dirichlet distribution with three variables.\nFigure 1.5: A Three-Variable Dirichlet Distribution with Various \\(\\alpha\\) Parameters\nAs a result, one can view the process of sampling from a Dirichlet distribution as picking a point on this simplex, where each point has a different likelihood of being picked.\nSince we are picking a point on the simplex, and each point \\(p\\) on the simplex has the property that \\(\\sum p_i = 1,\\) then one can view our samples as priors to a categorical distribution in \\(k\\) variables. For this reason, the Dirichlet distribution can be viewed as a distribution of distributions.\nBayesian Inference Having discussed the Dirichlet distribution, we can now turn our attention to Bayesian inference, which will be critical in understanding Latent Dirichlet Allocation. We begin by recalling the definition of Bayes' theorem:\nTheorem 2.3 Bayes' Theorem is stated as follows:\n$$P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B)}$$ where \\(A\\) and \\(B\\) are events and \\(P(B) \\neq 0.\\)\nProof: This is an immediate result from the definition of conditional probability, which states \\(P(B \\mid A) = P(A \\cap B)/P(A)\\) if \\(P(A) \\neq 0.\\) We have \\(P(B \\mid A)P(A) = P(A \\cap B),\\) then \\(P(A \\cap B)/P(B) = P(A \\mid B). \\) \u0026#9724;\nBayes' theorem states that we can compute the probability of \\(A\\) occurring given \\(B\\) if we know the probabilities of \\(A\\) occurring, \\(B\\) occurring, and \\(B\\) occurring given \\(A.\\) If we subscribe to the Bayesian interpretation of probability (that probabilities represent one's degree of subjective uncertainty), this provides us with a framework to perform belief updating, or the manner in which to change our degree of uncertainty about an event as we receive new information.\nConsider the process of using Bayesian inference to determine if a coin is weighted towards heads. One way to determine this is to flip the coin a number of times and record the results. If the results tend towards heads, we have good reason to believe that the coin is likely weighted. Mathematically, this can be formed as a problem of parameter estimation, or trying to find a parameter for our chosen distribution which is assumed to best fit the observed data. A useful probability distribution function to model this is the binomial distribution, defined as:\n$$ f(k,n,p) = \\binom{n}{k} p^k(1-p)^{n-k} $$\nWith a frequentist approach, we would perform a number of trials \\(X_1, \\cdots, X_n ,\\) create a likelihood function \\(\\mathcal{L}(p | X_1, \\cdots, X_n)\\) (which is the joint PDF of \\(X_1, \\cdots, X_n\\) with \\(p\\) as a parameter), then attempt to maximize it using points where \\(d\\mathcal{L}/dp = 0.\\)\nUsing a Bayesian approach, we would like to instead model our changing belief of the value of what \\(p\\) is likely to be as we get new batches of samples, through the use of Bayes' theorem. A core difference between estimating a parameter with an approach like maximum likelihood estimation and Bayesian inference is how the parameter in question is characterized. In the former, the parameter is an unknown constant, determined by means of an optimization procedure performed on the derivative of the likelihood function. In the latter, we treat the parameter with uncertainty, applying a probability distribution to it. For concreteness, consider modeling our prior \\(\\theta \\sim \\mathcal{N}(\\mu_0, \\sigma_0^2) .\\) By doing this, we are essentially stating that before any trials, we believe that \\(\\theta \\) is most likely the value \\(\\mu_0,\\) but could be something else. For example, it could be \\(\\mu_0 - \\sigma_0 ,\\) but we believe that's less likely to be the case, with the probability of each possible value of \\(\\theta \\) modeled by our distribution \\(\\mathcal{N}(\\mu_0, \\sigma_0^2) .\\)\nThrough Bayesian inference, we will be iteratively refining the parameters of our distribution based on the results of our trials so that at each new step \\(i ,\\) our new set of parameters \\(\\mu_i, \\sigma_i^2 \\) generate a distribution more in line with the data we observe.\nBefore we continue, we must make a note regarding the marginal likelihood \\(P(B) \\) within Bayes' theorem. Recall that this is the probability an event will occur independent of some other event. For instance, if \\(P(B|A) \\) is the probability of thunder occurring given rain, then \\(P(B) \\) is the probability of thunder occurring regardless as to if it is raining or not.\nThis acts as a normalizing constant, ensuring that the posterior distribution is in fact a legitimate probability distribution that adheres to Kolmogorov's axioms. However, this distribution can be difficult to compute in many circumstances, as marginalizing out a parameter typically requires us to sum or integrate over all possible parameter values, leading to an often intractable calculation. Moreover, in many cases we only care about certain relative aspects of the distribution, such as the location of the peak of the posterior (maximum a posteriori estimate). Since the application of a constant relative to \\(\\theta \\) is a monotonic function, it does not change where this peak would be, therefore we are able to disregard it in those circumstances. This is not to say that the marginal probability is unimportant. If we need a true probability distribution from Bayes' theorem, we need to compute it. If it is challenging to calculate directly, there are fortunately many ways to estimate it, such as through Monte Carlo Markov Chain methods or variational inference, the latter of which we will be discussing later in this post.\nA question naturally arises at this point: given our statistical model for the data, which distribution for the prior probability do we choose? We can answer this question by describing the characteristics of what would make for a good prior distribution:\nThe support of the prior distribution must match the range of the parameter in question. In the case of the binomial distribution, \\(p \\in [0,1] .\\) Therefore, any distributions with supports outside of that range are not appropriate to use. We would like the distribution to be as flexible as possible. Ideally, the distribution should be mathematically convenient to work with. Specifically, we would like it to be a conjugate prior to our model of the data, which is a prior distribution that when used in conjunction with the likelihood function within Bayes' theorem produces a posterior within the same distribution family as the prior. The benefit of this is that we do not have to determine what distribution we have to evaluate each time and by extension change our methods, as we remain in the same family. For the binomial distribution, a prior distribution which exhibits these characteristics is the beta distribution. Most importantly, it has a support of \\([0,1] .\\) Furthermore, the distribution is flexible in its shape, ranging from bimodality to a unimodal bell curve. Finally, the beta distribution is a conjugate prior to the binomial distribution, as: $$P(\\theta | D) \\propto P(D | \\theta) P(D | \\theta) = \\binom{n}{k} \\theta^k(1-\\theta)^{n-k} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{\\alpha - 1}(1-\\theta)^{\\beta - 1} = $$ $$\\binom{n}{k} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)} \\theta^{k+\\alpha - 1}(1-\\theta)^{n+\\beta-k-1}$$ Bayesian Inference with the Dirichlet Distribution Consider the process of picking a categorical prior from the Dirichlet distribution:\n$$\\theta \\sim Dir(\\alpha), \\qquad \\theta = [\\theta_1, \\cdots, \\theta_k], \\qquad \\sum_{i = 1}^k \\theta_i = 1, \\qquad p(\\theta|\\alpha) \\propto \\prod_{j=1}^k \\theta_j^{\\alpha_j - 1}$$ Now, consider grabbing some data from the categorical distribution parameterized by \\(\\theta.\\) \\( D = \\lbrace x_1, \\cdots x_n \\rbrace \\sim Cat(\\theta) \\) where \\( x_i \\in \\lbrace 1, \\cdots, k\\rbrace.\\)\n$$p(D|\\theta) = \\prod_{i=1}^n P(X_i = x_i|\\theta) = \\prod_{i=1}^n \\theta_{x_i}$$\nSince \\(\\theta_{x_i} = \\prod_{j=1}^{k} \\theta_j^{\\mathbb{I}(x_i = j)}\\) we can rewrite this as:\n$$p(D|\\theta) = \\prod_{i=1}^n\\prod_{j=1}^{k} \\theta_j^{\\mathbb{I}(x_i = j)} = \\prod_{j=1}^{k} \\theta_j^{\\sum \\mathbb{I}(x_i = j)} = \\prod_{j=1}^{k} \\theta_j^{c_j}$$ Since \\(p(D|\\theta)\\) and \\(p(\\theta)\\) come from the same family of distributions, we can say that \\(p(\\theta)\\) is a conjugate prior to the posterior distribution \\(p(\\theta|D).\\) By Bayes' Rule, we know that \\(p(\\theta|D) \\propto p(D|\\theta)p(\\theta).\\) Writing out the multiplication explicitly yields: $$\\prod_{i=1}^{k} \\theta_i^{c_i} \\prod_{j=1}^k \\theta_j^{\\alpha_j - 1} \\mathbb{I}(i=j) = \\prod_{j=1}^k \\theta_j^{c_i + \\alpha_j - 1} $$\nThis provides us with an update rule so that we may iteratively pick a categorical prior, observe data, and then update our Dirichlet prior. We now have a means to learn from data while maintaining the same kind of distribution.\nLatent Dirichlet Allocation Having discussed the Dirichlet distribution and how we can perform Bayesian inference with it, we can now turn our attention to Latent Dirichlet Allocation as a primer for the Embedded Topic Model. To start, we make the following assumptions:\nLet \\(D\\) be a corpus of documents, \\(K\\) a list of topics, and \\(W\\) a dictionary of words. Let some document \\(d_i \\in D\\) consist of \\(n\\) words and each word \\(w_{i,j}\\) be assigned to a topic \\(k_{i,j} \\in K.\\) In addition, suppose we can only observe the words in each document, and don't have information about topic assignments.\nIf we randomly pick a word in the document, the probability that we pick the particular word \\(w_p\\) is dependent on the categorical distribution given by the total amount of times \\(w_p\\) is present in the document divided by the total amount of words in the document.\nLikewise, the probability of the word we picked being assigned to a particular topic \\(K_p\\) is given by the amount of words in the document that that topic was assigned to divided by the total amount of words in the document; also a categorical distribution.\nIn human languages, we intuitively know that particular words tend to be coupled to topics. For instance, if we're reading an article about pets, we're likely to find many instances of the words cat or dog, but not many instances of algorithm.\nThe goal of LDA is to be a statistical model which sets up a structure relating words to latent variables (topics) with the ability to learn relationships based on data while maintaining the categorical distributions described above. The reason why LDA employs the Dirichlet distribution is to be able to perform Bayesian inference with the categorical distribution as a prior and posterior. LDA is a hierarchical model; in particular, a mixture model. We will first provide the intuition for mixture models then provide a definition below.\nMixture Models Figure 3.1: Clusters That Could Be Represented by a Gaussian Mixture Model\nConsider the problem of clustering a set of data. In this case, we're given a set of data, and are assuming that clusters exist within it. These clusters would be considered our latent variables. Following the clustering performed in the above image, it can be inferred that the data contained in each cluster adheres to a Gaussian distribution with respect to that particular cluster.\nMixture models aim to describe this entire dataset as a linear combination of the distributions of each individual cluster, rather than attempt to describe the entire dataset using a more complicated probability distribution with three peaks.\nThe key to understanding why this is possible is through marginalization. Suppose that \\(p(x)\\) represents the probability of selecting a point, and \\(p(x|c)\\) be the probability of selecting a point given some cluster. By marginalization, we know that:\n$$p(x) = \\sum_{c \\in C} p(x|c)p(c) $$\nThus, \\(p(x)\\) can be represented as a sum on a simpler joint distribution between observed and latent variables (in our case, clusters).\nWe will now give the non-Bayesian definition of a mixture model.\nDefinition 3.2: A mixture model can be described as follows:\n\\(K \\) is the number of mixture components \\(N \\) is the number of observations \\(\\theta_{i=1, \\cdots, K} \\) is the parameter of distribution of observation of component \\(i \\) \\(\\phi_{i=1, \\cdots, K} \\) is the mixture weight of component \\(i \\) (here, \\(\\sum_{i=1}^{K}\\phi_i = 1 \\)) \\(z_{i=1, \\cdots, N} \\) is the component of observation \\(i \\) \\(x_{i=1, \\cdots, N} \\) is observation \\(i \\) \\(F(x|\\theta)\\) is the probability distribution of an observation \\(z_{i=1, \\cdots, N} \\sim \\mbox{Cat}(\\phi) \\) \\(x_{i=1, \\cdots, N} \\sim F(\\theta_{z_i})\\) In addition, we provide a definition for a Gaussian Mixture Model:\nDefinition 3.3: A Gaussian mixture model can be described as follows:\n\\(K \\) is the number of mixture components \\(N \\) is the number of observations \\(\\theta_{i=1, \\cdots, K} = \\lbrace \\mu_{i=1, \\cdots, K}, \\sigma^2_{i=1, \\cdots, K} \\rbrace\\) \\(\\phi_{i=1, \\cdots, K} \\) is the mixture weight of component \\(i \\) (here, \\(\\sum_{i=1}^{K}\\phi_i = 1 \\)) \\(z_{i=1, \\cdots, N} \\) is the component of observation \\(i \\) \\(x_{i=1, \\cdots, N} \\) is observation \\(i \\) \\(z_{i=1, \\cdots, N} \\sim \\mbox{Cat}(\\phi) \\) \\(x_{i=1, \\cdots, N} \\sim \\mathcal{N}(\\mu_{z_i},\\sigma_{z_i} )\\) Learning with Latent Dirichlet Allocation Finally, we give the definition of Latent Dirichlet Allocation.\nDefinition 3.4: Latent Dirichlet Allocation may be described as follows:\nChoose two vectors \\(\\alpha \\) and \\(\\beta ,\\) where \\(|\\alpha| \\) is the number of topics we have and \\(|\\beta| \\) is the length of the vocabulary. For each document \\(d_i ,\\) choose \\(\\theta_i \\sim \\mbox{Dir}(\\alpha) \\) and for each topic, \\(\\phi_k \\sim \\mbox{Cat}(\\phi_{t_{i,j}}) .\\) Let \\(d_i\\) be a document with \\(n\\) words. For each word position \\(w_{i,j}\\) in a document, choose a topic \\(t_{i,j} \\sim Cat(\\theta_i)\\) and a word \\(w_{i,j} \\sim Cat(\\phi_{t_{i,j}}).\\) A natural assumption to make is that any particular document will only have a handful of topics contained in it, which would make our \\(\\alpha \\) vector sparse. Using the Bayesian procedure described above for the Dirichlet distribution, we can fit this model to a dataset of documents.\nWord Embeddings When attempting to make sense of natural language semantics, it’s useful to develop a representation of the vocabulary with relational properties. One way to do this is by means of a semantic network, an example of this being WordNet.\nDefinition 2.3: A semantic network can be defined as a directed graph \\(G = (V, E),\\) where \\(V\\) represents a set of nodes or vertices, and \\(E\\) represents a set of directed edges. Each node in \\(V\\) corresponds to a concept or entity, and each directed edge in \\(E\\) represents a relationship or link between two nodes.\nFormally, \\(V = \\lbrace v_1, v_2, ..., v_n \\rbrace\\) is the set of nodes, and \\(E = \\lbrace (v_i, v_j) | v_i, v_j \\in V \\rbrace \\) is the set of directed edges. The directed edge \\((v_i, v_j)\\) signifies that there is a relationship from node \\(v_i\\) to node \\(v_j.\\) The edges can be labeled to indicate the type of relationship, such as is-a, part-of, causes, or any other relevant semantic connection.\nSemantic networks excel at encoding hierarchical (is-a) relationships. For instance:\n$$\\mbox{canary} \\overset{is-a}{\\longrightarrow} \\mbox{bird}$$ $$\\mbox{Jane} \\overset{is-a}{\\longrightarrow} \\mbox{human}$$ $$\\mbox{Human} \\overset{is-a}{\\longrightarrow} \\mbox{mammal}$$ Due to its graph structure, transitive logic is easy. We can simply chain together facts to determine that Jane is a human and human is a mammal, thus Jane is a mammal.\nIn addition, it’s easy to extend the network to include and reason with particular instances of words, as well as possessive (has) and descriptive (is) relationships:\n$$\\mbox{Jane} \\overset{has}{\\longrightarrow} \\mbox{eyes} \\overset{is}{\\longrightarrow} \\mbox{brown}$$ $$\\mbox{Mary} \\overset{has}{\\longrightarrow} \\mbox{eyes} \\overset{is}{\\longrightarrow} \\mbox{brown}$$ $$\\mbox{John} \\overset{has}{\\longrightarrow} \\mbox{eyes} \\overset{is}{\\longrightarrow} \\mbox{green}$$ human(jane). human(mary). human(john). eye_color(jane, brown). eye_color(mary, brown). eye_color(john, green). ?- human(X), eye_color(X, brown), human(Y), eye_color(Y, brown), X \\= Y. X = jane, Y = mary; Prolog code which describes such a semantic network.\nAssuming that Jane, Mary, and John are connected to human by a is-a relationship, we could say that the abstract node human can have green or brown eyes, and quantify that there exists at least two humans with brown eyes, and at least one with green eyes. This kind of relationship is very natural to us, as we tend to think of a word as a symbolic link to something else. However, semantic networks are not without their downsides:\nIt can be hard to generate a network; in the past, these had to be hand-built. However, strides are being made towards doing this automatically with ontology learning. It is not easy to capture analogous reasoning within a semantic network. One might wonder how you can use a network to answer: Man is to woman as king is to what? Semantic networks usually do not by themselves capture degrees of synonymy or intensity of words. Good and incredible could be synonymous in a network, but they don’t have the same semantic meaning. It feels like good and alright should have a closer similarity measure than good and incredible. To deal with the latter two issues, one solution is to come up with a real-valued measurement between words. One option is to learn representations these words in a continuous metric space. Considering words to be vectors in a vector space is a natural choice for the following reasons:\nVectors are easy to encode in numerical form, and most hardware has been optimized for linear algebra, allowing for fast computation. Vector addition/subtraction provides an easy mechanism for how we might perform analogical reasoning: \\(v_{woman} + v_{king} - v_{man} \\approx v_{queen}.\\) There exists a variety of measures for similarity, such as cosine similarity given by \\((A \\cdot B)/(||A||\\cdot||B||) \\in [-1,1].\\) The amount of dimensions we choose for our vector space to have is a hyperparameter we can optimize for, giving us more flexibility. Definition 2.4: Such a model that represents the semantic content of words as vectors in a vector space is called a word embedding model.\nFollowing a training procedure which generates vectors for words based off of a text corpus, each axis can be said to represent a latent meaning associated with it, similar to how certain hidden units in a neural network can represent latent features. The larger the dimension of the vector space, the more space there is for features.\nGenerating Word Embeddings There are a variety of ways we can use to generate word embeddings. Common methods include:\nNaive Softmax Hierarchical Softmax Negative Sampling Co-occurrence Matrix Methods Singular Value Decomposition Global Vectors We'll focus on singular value decomposition in the following paragraphs, since it is easy to understand. We begin by constructing an object called co-occurrence matrix, which is a count of how often words appear in the same reference frame. If we consider the reference frame to be two words, then we’ll say that one word appears in the presence of another if it’s at most a distance of two words from it in a text corpora. Reference frames are something we select, and can be considered to be another hyperparameter to optimize. These can be a specific word distance, an entire sentence, etc. The idea behind a co-occurrence matrix is that words that are used in close proximity to one another are likely to have meanings that are also in close proximity.\nConsider the following text: My car is having engine problems. Typically, the engine of a car lasts for about a quarter of a million miles. For brevity, we're only going to consider the nouns in this sentence, and we will use a window length of 4. The following is a co-occurrence matrix:\n$$\\begin{array}{ccccccc} \u0026 \\text{million} \u0026 \\text{problems} \u0026 \\text{engine} \u0026 \\text{car} \u0026 \\text{quarter} \u0026 \\text{miles} \\\\\\ \\text{million} \u0026 0 \u0026 0 \u0026 1 \u0026 1 \u0026 1 \u0026 1 \\\\\\ \\text{problems} \u0026 0 \u0026 0 \u0026 1 \u0026 1 \u0026 0 \u0026 0 \\\\\\ \\text{engine} \u0026 1 \u0026 1 \u0026 0 \u0026 2 \u0026 1 \u0026 1 \\\\\\ \\text{car} \u0026 1 \u0026 1 \u0026 2 \u0026 0 \u0026 1 \u0026 1 \\\\\\ \\text{quarter} \u0026 1 \u0026 0 \u0026 1 \u0026 1 \u0026 0 \u0026 1 \\\\\\ \\text{miles} \u0026 1 \u0026 0 \u0026 1 \u0026 1 \u0026 1 \u0026 0 \\\\\\ \\end{array}$$ By the existence theorem of singular value decomposition, we know that any square symmetric matrix \\(M \\) has a decomposition \\(M = U \\Sigma V^T ,\\) where \\(U \\) and \\(V \\) are unitary matrices, and \\(\\Sigma \\) is a rectangular diagonal matrix with positive entries. The result of SVD for the above matrix is as follows:\n$$U = \\begin{bmatrix} -0.381 \u0026 -0.348 \u0026 0.260 \u0026 0.000 \u0026 -0.783 \u0026 -0.230 \\\\\\ -0.218 \u0026 0.726 \u0026 0.653 \u0026 0.000 \u0026 0.000 \u0026 0.000 \\\\\\ -0.509 \u0026 0.235 \u0026 -0.431 \u0026 -0.707 \u0026 0.000 \u0026 0.000 \\\\\\ -0.509 \u0026 0.235 \u0026 -0.431 \u0026 0.707 \u0026 0.000 \u0026 0.000 \\\\\\ -0.381 \u0026 -0.348 \u0026 0.260 \u0026 0.000 \u0026 0.591 \u0026 -0.563 \\\\\\ -0.381 \u0026 -0.348 \u0026 0.260 \u0026 0.000 \u0026 0.192 \u0026 0.794 \\end{bmatrix} $$ $$ \\Sigma = \\begin{bmatrix} 4.673 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \\\\ 0.000 \u0026amp; 0.648 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \\\\ 0.000 \u0026amp; 0.000 \u0026amp; 1.321 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \\\\ 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 2.000 \u0026amp; 0.000 \u0026amp; 0.000 \\\\ 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 1.000 \u0026amp; 0.000 \\\\ 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 1.000 \\end{bmatrix}$$\n$$V^T = \\begin{bmatrix} -0.381 \u0026amp; -0.218 \u0026amp; -0.509 \u0026amp; -0.509 \u0026amp; -0.381 \u0026amp; -0.381 \\\\ -0.348 \u0026amp; 0.726 \u0026amp; 0.235 \u0026amp; 0.235 \u0026amp; -0.348 \u0026amp; -0.348 \\\\ -0.260 \u0026amp; -0.653 \u0026amp; 0.431 \u0026amp; 0.431 \u0026amp; -0.260 \u0026amp; -0.260 \\\\ 0.000 \u0026amp; 0.000 \u0026amp; 0.707 \u0026amp; -0.707 \u0026amp; 0.000 \u0026amp; 0.000 \\\\ 0.783 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; -0.591 \u0026amp; -0.192 \\\\ 0.230 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.000 \u0026amp; 0.563 \u0026amp; -0.794 \\end{bmatrix} $$\nTo get our corresponding word vectors, we may take the \\(i^{th} \\) column vector of \\(V^T \\) or the \\(i^{th} \\) row vector of \\(U \\) for the corresponding word in the \\(i^{th} \\) column of the co-occurrence matrix. We can then do a quick check to see if these are sensible vectors by attempting to compute similarity via the dot product (or cosine similarity). We will choose the former, and take the column vectors of \\(V^T .\\) As a side note, the matrices \\(U \\) and \\(V^T \\) are called vocabulary embeddings, since they contain the vectors of every word in our vocabulary.\nNow, let's attempt to compute some similarity metrics. It should be the case that engine and car are more similar than problems and million. For the former, our similarity value is \\(0.000218 \\) whereas for the latter, the value is \\(0.00019 .\\) The difference is quite small due to limited data, but it is there.\nThe Embedded Topic Model Having discussed word embeddings, we now have enough background to begin covering content specific to the embedded topic model. We will begin by looking at the particular word embedding used in the ETM.\nContinuous Bag of Words Continuous Bag of Words (CBOW) is an embedding method which is based on the problem of predicting a word in a document given its surrounding words. That is to say, if we are given the sentence The quick brown fox ______ over the lazy dog., our task is to determine what word to use to fill in the blank. To solve this problem, we can learn embeddings such that words that are near each other have similar vector representations, then use these vectors to help us predict the missing word. Recall our notion of a vocabulary embedding from the last section, which is our set of learned vector representations of words concatenated into a matrix: $$\\rho = \\begin{bmatrix} \\rho_{1,1} \u0026amp; \\rho_{1,2} \u0026amp; \\cdots \u0026amp; \\rho_{1,m} \\\\ \\rho_{2,1} \u0026amp; \\rho_{2,2} \u0026amp; \\cdots \u0026amp; \\rho_{2,m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\rho_{n, 1} \u0026amp; \\rho_{n,2} \u0026amp; \\cdots \u0026amp; \\rho_{n,m} \\end{bmatrix}$$\nHere, we have \\(m\\) words, each of which are represented by \\(n \\times 1\\) vectors.\nFor an instance of CBOW, let our context window be \\(\\alpha_1, \\cdots, \\alpha_{k-1}, \\alpha_k, \\alpha_{k+1}, \\cdots, \\alpha_n .\\) \\(W = \\lbrace \\alpha_1, \\cdots, \\alpha_{k-1}, \\alpha_{k+1}, \\cdots, \\alpha_n \\rbrace \\) will be the set of words we take into consideration when trying to determine \\(\\alpha_k .\\) Our approach to this will be to first average the word vectors associated with every element in \\(W ,\\) then perform matrix multiplication with our vocabulary embedding. Finally, we apply a softmax function to the resulting vector to get a corresponding vector with a norm of \\(1.\\) This gives us a parameter for a categorical distribution, which we can then sample from to generate a prediction for the missing word.\nSince our learning procedure should have generated vectors which are similar if they are nearby in a corpus, we would expect that the missing word should have a similar embedding assuming the context window follows the distribution of the corpus we trained on. Thus, we should expect that the resulting index of the vector generated from multiplying the average context word embedding with the vocabulary embedding should have a high value relative to other values in the vector. When fed into a softmax function then taken as a categorical distribution, this translates to a high probability a word that is commonly seen amongst the averaged words will be chosen.\nMathematically speaking:\n$$v_k = \\frac{1}{n-1} \\sum_{\\alpha_i \\in W} \\alpha_i $$\n$$\\mbox{softmax}(\\vec{k})_i = \\frac{e^{k_i}}{\\sum _{j=1}^n e^{k_j}}$$\n$$\\alpha_k \\sim \\mbox{Cat}(\\mbox{softmax}(\\rho^Tv_k))$$\nThe Logistic Normal Distribution In this new model, we depart slightly from our standard Dirichlet distribution and introduce something called the logistic normal distribution, which is the multivariate normal distribution with a softmax function applied to its samples. By itself, the multivariate normal distribution is a generalization of our familiar bell curve to multiple variables, and satisfies all of the typical axioms of a probability distribution as we would expect.\nDefinition 6.1: The PDF of the multivariate normal distribution is defined as:\n$$f_X(x_1,\\cdots,x_n) = \\frac{\\exp(-\\frac{1}{2} (\\vec{x} - \\vec{\\mu})^T \\Sigma^{-1}(\\vec{x} - \\vec{\\mu})}{\\sqrt{(2\\pi)^n \\det(\\Sigma)}}$$ where \\(\\vec{\\mu} \\in \\mathbb{R}^n\\) and \\(\\Sigma \\in \\mathbb{R}^{n \\times n}\\) (\\(\\Sigma\\) is also positive-semidefinite).\nEach element \\(\\vec{x}_i\\) in the vector that is sampled from the distribution can be thought of as a random variable from its own (single-variable) normal distribution, each of which being related to one another the covariance matrix \\(\\Sigma.\\) Understanding \\(\\vec{\\mu}\\) is straightforward as \\(\\vec{\\mu}_i\\) corresponds to the mean of \\(\\vec{x}_i,\\) but what of the covariance matrix?\n\\(\\Sigma\\) has the property that: $$\\Sigma_{X_i,X_j} = cov(X_i, X_j) = E[(X_i - E[X_i])(X_j - E[X_j])]$$ Since \\(cov(X_i, X_i) = var(X_i),\\) the diagonal elements of the matrix are the variance of each variable. Naturally, the matrix is symmetric.\nAs stated before, a draw from this distribution is a sample from the multivariate normal distribution normalized to a unit vector via the softmax function. So: $$X \\sim \\mathcal{LMN}(\\mu, \\Sigma) = \\mbox{softmax}(Y) \\mbox{ where } Y \\sim \\mathcal{MN}(\\mu, \\Sigma)$$ Definition of the Embedded Topic Model We may now define the embedded topic model:\nLet the \\( L \\times V \\) vocabulary embedding be \\( \\rho ,\\) where the column \\(\\rho_v\\) is the embedding of \\(v.\\) Under the Embedded Topic Model, we generate the \\(d^{th}\\) document as follows: Draw Topic Proportions: \\( \\theta_d \\sim \\mathcal{LMN}(0,I),\\) where \\(\\theta_d = \\mbox{softmax}(\\delta_d)\\) and \\(\\delta_d \\sim \\mathcal{MN}(0,I) \\) For each word position \\(n\\) in the document \\(d:\\) Draw topic assignment \\(z_{dn} \\sim \\mbox{Cat}(\\theta_d) \\) Draw the word \\(w_{dn} \\sim \\mbox{softmax}(\\rho^T \\alpha_{z_{dn}})\\) where \\(\\alpha_k \\) is the \\(k^{th} \\) topic embedding. Parts 1 and 2a are similar to LDA, with the exception of the distribution we draw from (with parameters being the zero vector and the identity matrix) for the topic proportions. Step 2b is different since it generates a categorical prior based on the correspondence between words in our word embedding matrix and the embedding of the chosen topic, resembling the continuous bag of words model described earlier.\nThus, the two primary differences between ETM and LDA are:\nRather than each topic being a distribution over words, it is just a learned vector. Our metric for determining how much a word belongs to a topic is just an inner product. The a priori assumption that LDA took on where a document is heavily represented by a few topics is not present in the ETM. Bayesian Inference with the ETM We now have to consider how we might learn the parameters of the ETM (the topic and vocabulary embeddings) from a set of documents \\(A = \\lbrace w_1, \\cdots, w_D \\rbrace.\\) We begin by considering the marginal log-likelihood:\n$$\\mathcal{L}(\\alpha, \\rho) = \\sum_{d=1}^D \\log p(w_d \\mid \\alpha, \\rho)$$ This represents the probability of picking the words in our document given the vocabulary embedding and topic embedding matrices with the topic proportions marginalized out. This comes from the likelihood \\(p(W \\mid \\alpha, \\rho)\\) where \\(W\\) is the set of words in our documents. Since each document is independent from the other:\n$$p(W \\mid \\alpha, \\rho) = \\prod_{d=1}^D p(w_d \\mid \\alpha, \\rho) \\propto \\sum_{d=1}^D \\log p(w_d \\mid \\alpha, \\rho)$$ where \\(w_{d,n}\\) is the \\(n^{th}\\) word position in the \\(d^{th}\\) document. If we want to compute this, we have to include the previously marginalized topic sampled from a logistic normal distribution: $$p(w_d \\mid \\alpha, \\rho) = \\int p(\\delta_d) \\prod_{n=1}^{N_d} p(w_{d,n} \\mid \\alpha, \\delta_d ,\\rho) d \\delta_d$$ Since the words are independent, we can split this into a product of probabilities. Furthermore, we can write:\n$$p(w_{d,n} \\mid \\alpha, \\delta_d, \\rho) = \\sum_{k=1}^K \\theta_{d,k} \\beta_{k,w_{d,n}}$$ where \\(\\beta_{k,w_{d,n}} = \\mbox{softmax}(\\rho^T \\alpha_k)|_{ w_{d,n}}\\)\nThis is because we know that the \\(w_{d,n} \\sim \\mbox{softmax}(\\rho^T \\alpha_{z_{d,n}}).\\) Since \\(\\theta_{d,k}\\) acts as a weight and \\(\\sum_{k=1}^K \\theta_{d,k} = 1,\\) we're simply computing the probability of a word being chosen based on a weighted inner product. So, our explicit likelihood function is:\n$$\\sum_{d=1}^D \\log \\Bigg( \\int p(\\delta_d) \\prod_{n=1}^{N_d} \\Bigg(\\sum_{k=1}^K \\theta_{d,k} \\beta_{k,w_{d,n}} \\Bigg) d\\delta_d \\Bigg)$$ Approximating using Variational Inference This integral is intractable due to the fact that as you add documents, the space of possible variable assignments increases exponentially. To remedy this, we leverage variational inference. This attempts to approximate intractable problems of Bayesian inference by casting them as optimization problems.\nThe basic idea is this: Suppose we are given a probability distribution \\(p\\) which proves intractable to estimate. We can take a class of probability distributions \\(Q,\\) and generate a distribution from that class \\(q \\in Q\\) which is most similar to \\(p\\) out of all other \\(q'\\in Q.\\)\nVariational approaches usually don't find the globally optimal solution, rather a local maxima. We frame it as an optimization problem of two parameters; the model parameters of \\(q\\) as well as the variational parameters \\(v\\) which describe the similarity between \\(q\\) and \\(p.\\)\nTo set up a variational inference scheme for our problem, first consider a class of distributions for our untransformed topic proportions: \\(q(\\delta_d; w_d, v)\\) as a Gaussian distribution whose mean and variance are sampled from something called an inference network, which is a neural network parameterized by \\(v.\\) This network takes in the words of a document as a vector input and outputs the mean and variance.\nNote that since our neural network has a fixed input length, we have to represent each document as a vector of the same length, regardless of the size. As a result, we cluster our words into a bag-of-words representation. We are going to be optimizing a function called ELBO, or Evidence Lower Bound. The evidence is synonymous with the value of our likelihood function at a particular set of parameters.\nThe ELBO function is related to the evidence in that it gives a lower bound on the likelihood function when evaluated at specific parameters. That is to say, the value we get when evaluating ELBO at particular parameters provides a guarantee that the likelihood function is not below a certain value when evaluated at those same parameters.\nSince we can't maximize the evidence directly, we can instead work on maximizing the ELBO, which indirectly provides us evidence that our choice of parameters are good. The difference between the ELBO and our likelihood function can be proven to just be the Kullback-Leibler Divergence between the two distributions \\(q\\) and \\(p.\\) For continuous distributions, this is defined to be:\n$$D_{KL}(p || q) = \\int_{-\\infty}^\\infty p(x)\\log\\frac{p(x)}{q(x)}dx $$ so \\(D_{KL}(p || q) = evidence - ELBO \\implies ELBO = evidence - D_{KL}(p || q).\\)\n$$\\mathcal{L}(\\alpha, \\rho, v) = \\sum_{d=1}^D \\sum_{n=1}^{N_d} \\mathbb{E}_ q[\\log p (w_{dn} \\mid \\delta_d, \\rho, \\alpha)] - \\sum_{d=1}^D KL(q(\\delta_d; w_d, v) || p(\\delta_d)) $$ We then optimize \\(\\mathcal{L}(\\alpha, \\rho, v)\\) with respect to the variational and model parameters. This can be done through a variety of methods; in the paper, the authors use the stochastic optimization with Monte Carlo approximations. We will not go into the details of this, and instead end the post here.\nReferences [1] Blei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3.Jan (2003): 993-1022.\n[2] Dieng, Adji B., Francisco JR Ruiz, and David M. Blei. \"Topic modeling in embedding spaces.\" Transactions of the Association for Computational Linguistics 8 (2020): 439-453.\n[3] Bishop, Christopher M., and Nasser M. Nasrabadi. Pattern recognition and machine learning. Vol. 4. No. 4. New York: springer, 2006.\n[4] Manning, Christopher, and Hinrich Schutze. Foundations of statistical natural language processing. MIT press, 1999.\n[5] Princeton University \"About WordNet.\" WordNet. Princeton University. 2010. ","permalink":"https://MikeLilley.github.io/posts/topic_modeling_embedding_spaces/","summary":"In this post, we will be going over the paper Topic Modeling in Embedding Spaces by Dieng et al., with an extended explanation of the background content.","title":"Topic Modeling in Embedding Spaces"},{"content":" Experience USS Vision, Inc. Position: Machine Vision Engineer\nLocation: Livonia, MI, USA\nDates of Employment: September 2021 - Present\nCreate bespoke real-time machine vision software in Python and C++ deployed on Linux devices for automatic defect detection during various manufacturing processes.\nLed the software development of a system that visually inspects for various types of defects in sheet metal panels. When installed on a major automotive OEM's stamping press line, defect outflow was reduced by 91.2%.\nLed the software development of a real-time quality checking system for laser-engraved vehicle headlamps, defect detection performance exceeded 99%.\nParticipated in the creation of custom software for Matrox Iris GTX cameras, adding the capability to load in arbitrary custom ONNX model files to use for inference. Create custom machine learning architectures centered on high-performance, both in accuracy and speed. Employ tools such as OpenCV, Scikit-Image, PyTorch, TensorFlow, and Scikit-Learn for image processing, data analysis, machine learning, and anomaly detection.\nUse Docker, Kubernetes, and Apache Kafka for container orchistration and inter-process communication.\nParallelize data pipelines across many CPU cores and high-bandwidth distributed systems, as well as deploy on Nvidia hardware for accelerated processing capabilities.\nDeploy and manage SQL databases and web servers for KPI records.\nInterface with industrial hardware such as GigE cameras and programmable logic controllers.\nServe as a technical contact for clients.\nUniversity of Michigan - Dearborn Position: Research Assistant, Tutor\nLocation: Dearborn, MI, USA\nDates of Employment: October 2020 - Present (RA), March 2018 - April 2021 (Tutor)\nLed a project which studied the application of concepts from computational topology to the problem of facial region segmentation, with our results later being published in IEEE ISM 2022; traveled solo to and gave a talk at the conference in Naples, Italy. (RA)\nUsed US congessional voting records to study graphical games in the context of game theory. Constructed and ran experiments to determine overall model fitness, analyzed the effects that pruning certain inputs had on the model, and determined if the behavior of a particular agent was better described by one model or another. (RA)\nRegularly met with other students to assist them with coursework. Tutored up to 400-level mathematics and computer science courses, as well as general physics. (Tutor)\nWoolf Aircraft Products, Inc. Position: Mechanical Drafter (Summer Internship)\nLocation: Romulus, MI, USA\nDates of Employment: May 2017 - August 2017\nDetailed tubing layouts in AutoCAD and Autodesk Inventor.\nInspected and pressure-tested components after being fabricated.\nEnviroSolutions, Inc. Position: Civil Drafter (Year-Long Co-Op)\nLocation: Westland, MI, USA\nDates of Employment: June 2016 - May 2017\nDetailed contour, plume, and analytical maps in AutoCAD to convey contamination in the groundwater and soil of Superfund sites.\nPublications M. Lilley, K. Das, K. Riani and M. Abouelenien, \"A Topological Approach for Facial Region Segmentation in Thermal Images,\" 2022 IEEE International Symposium on Multimedia (ISM), Italy, 2022, pp. 189-193, doi: 10.1109/ISM55400.2022.00042.\nEducation University of Michigan - Dearborn Majors: B.Sc. Computer Science and B.Sc. CIS Mathematics - Dual Degree\nLocation: Dearborn, MI, USA\nDates of Attendance: September 2017 - August 2021\nGPA: 3.56\nMathematics Coursework: Calculus I-III, Differential Equations, Introduction to Mathematical Proofs, Introduction to Linear Algebra, Advanced Linear Algebra, Number Theory, Cryptography, Abstract Algebra, Probability and Statistics, Topology, Numerical Analysis, Real Analysis, Differential Geometry, Algebraic Geometry\nComputer Science Coursework: Software Engineering, Data Structures and Algorithms, Algorithm Analysis and Design, Artificial Intelligence, Computational Learning, Computer Organization and Assembly Language, Natural Language Processing, Web Technology, Operating Systems, Computer Networking and Distributed Processes, Computer Vision\nCertifications A3 Automate - AIA Certified Vision Professional: Advanced (Granted September 2022)\nTechnical Skills Programming Languages: Python, C++, C, JavaScript, SQL, HTML/CSS, LaTeX, Markdown, Prolog, Common LISP, Haskell Technologies and Software Libraries: Computer Vision: OpenCV, Scikit-Image, Point Cloud Library (PCL) Machine Learning: PyTorch, Scikit-Learn, TensorFlow, Anomalib, ONNX, Nvidia TensorRT, OpenVINO Natural Language Processing: NLTK, SpaCy Scientific Computing: NumPy, SciPy, Jupyter Notebooks Containerization: Docker, Podman, Kubernetes Distributed Computing: ZeroMQ, Apache Kafka Computer Algebra Systems: Singular, SageMath Web Services: Flask, Hugo Operating Systems: Linux, Windows, BSD Methods: Machine Learning: Neural Networks (CNNs, Autoencoders, Transformers), Decision Trees, Ensemble Learning (AdaBoost, XGBoost, Random Forests, Gradient Boosting), Clustering Algorithms (K-Means, DBSCAN), Dimensionality Reduction (t-SNE), k-NN Computer Vision: Image Classification, Object Detection, Image Segmentation, Anomaly Detection, Histogram Analysis, Feature Descriptors (SIFT/SURF/ORB), Filtering, Contour and Edge Detection, Blob Analysis, 3D Point Cloud Analysis, Homography and Image Registration, Hough Transform, Fourier Analysis Natural Language Processing: Parsing, Part-of-Speech Tagging, Sentiment Analysis, Textual Entailment, Summarization, Topic Modeling Computing Environments: Distributed Systems, Real-Time Computing ","permalink":"https://MikeLilley.github.io/cv/","summary":"Experience USS Vision, Inc. Position: Machine Vision Engineer\nLocation: Livonia, MI, USA\nDates of Employment: September 2021 - Present\nCreate bespoke real-time machine vision software in Python and C++ deployed on Linux devices for automatic defect detection during various manufacturing processes.\nLed the software development of a system that visually inspects for various types of defects in sheet metal panels. When installed on a major automotive OEM's stamping press line, defect outflow was reduced by 91.","title":"Curriculum Vitae"}]